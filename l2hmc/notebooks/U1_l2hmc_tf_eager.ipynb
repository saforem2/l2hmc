{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2HMC for $U(1)$ Gauge Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T19:40:55.893485Z",
     "start_time": "2019-02-01T19:40:52.080120Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import i0, i1\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "#from l2hmc_eager import dynamics_eager as _l2hmc\n",
    "#from l2hmc_eager import gauge_dynamics_eager as l2hmc\n",
    "#from l2hmc_eager.neural_nets import *\n",
    "#from utils.distributions import GMM, gen_ring\n",
    "#from utils.jacobian import _map, jacobian\n",
    "from utils.data_utils import (\n",
    "    calc_avg_vals_errors, block_resampling, jackknife_err\n",
    ")\n",
    "#from u1_model_eager import *\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T19:40:57.116945Z",
     "start_time": "2019-02-01T19:40:56.567251Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'line_profiler'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-946a311175ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'load_ext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'line_profiler'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'load_ext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'memory_profiler'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~/miniconda3/envs/intelpy3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2129\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2130\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2131\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2132\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-65>\u001b[0m in \u001b[0;36mload_ext\u001b[1;34m(self, module_str)\u001b[0m\n",
      "\u001b[1;32m~/miniconda3/envs/intelpy3/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~/miniconda3/envs/intelpy3/lib/python3.6/site-packages/IPython/core/magics/extension.py\u001b[0m in \u001b[0;36mload_ext\u001b[1;34m(self, module_str)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodule_str\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mUsageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Missing module name.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextension_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'already loaded'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~/miniconda3/envs/intelpy3/lib/python3.6/site-packages/IPython/core/extensions.py\u001b[0m in \u001b[0;36mload_extension\u001b[1;34m(self, module_str)\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmodule_str\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mprepended_to_syspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                     \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                         print((\"Loading extensions from {dir} is deprecated. \"\n",
      "\u001b[1;32m~/miniconda3/envs/intelpy3/lib/python3.6/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~/miniconda3/envs/intelpy3/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "\u001b[1;32m~/miniconda3/envs/intelpy3/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~/miniconda3/envs/intelpy3/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'line_profiler'"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T19:41:12.458972Z",
     "start_time": "2019-02-01T19:41:04.088214Z"
    }
   },
   "outputs": [],
   "source": [
    "from l2hmc_eager import gauge_dynamics_eager as gde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T19:41:14.790840Z",
     "start_time": "2019-02-01T19:41:14.717543Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()\n",
    "tfe = tf.contrib.eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T19:41:16.318128Z",
     "start_time": "2019-02-01T19:41:16.243306Z"
    }
   },
   "outputs": [],
   "source": [
    "from lattice.gauge_lattice import u1_plaq_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T19:41:44.465525Z",
     "start_time": "2019-02-01T19:41:44.388051Z"
    }
   },
   "outputs": [],
   "source": [
    "from u1_model_eager import GaugeModelEager, train_one_iter\n",
    "#from gauge_model import GaugeModel, graph_step, train_one_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T19:41:47.957023Z",
     "start_time": "2019-02-01T19:41:47.889154Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.gauge_model_helpers import plot_run_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T19:41:49.402839Z",
     "start_time": "2019-02-01T19:41:49.336490Z"
    }
   },
   "outputs": [],
   "source": [
    "import utils.gauge_model_helpers as helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T19:41:50.442223Z",
     "start_time": "2019-02-01T19:41:50.371772Z"
    }
   },
   "outputs": [],
   "source": [
    "def autocorr(x):\n",
    "    result = np.correlate(x, x, mode='full')\n",
    "    result /= result[result.argmax()]\n",
    "    return result[result.size//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T19:41:50.977688Z",
     "start_time": "2019-02-01T19:41:50.906149Z"
    },
    "code_folding": [
     0,
     4,
     10,
     19
    ]
   },
   "outputs": [],
   "source": [
    "def graph_step(dynamics, samples, optimizer, loss_fn, \n",
    "               params,  global_step=None, hmc=False):\n",
    "    \"\"\"Perform a single training step using the compiled tensorflow graph.\n",
    "    \n",
    "    NOTE: \n",
    "        To be defunnable, the function cannot return an Operation, so the above\n",
    "        function is used for defun or eager, and this function is used in graph to be\n",
    "        able to run the gradient updates.\n",
    "    \"\"\"\n",
    "    clip_value = params.get('clip_value', 100)\n",
    "    loss, samples_out, accept_prob, grads = gde.loss_and_grads(\n",
    "        dynamics=dynamics,\n",
    "        x=samples,\n",
    "        params=params,\n",
    "        loss_fn=loss_fn,\n",
    "        hmc=hmc\n",
    "    )\n",
    "    \n",
    "    grads, _ = tf.clip_by_global_norm(grads, clip_value)\n",
    "    train_op = optimizer.apply_gradients(\n",
    "        zip(grads, dynamics.trainable_variables), global_step=global_step\n",
    "    )\n",
    "    \n",
    "    return train_op, loss, samples_out, accept_prob, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use GaugeModelEager for training L2HMC on $U(1)$ gauge lattice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T19:42:16.953103Z",
     "start_time": "2019-02-01T19:42:16.869237Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'time_size': 8,\n",
    "    'space_size': 8,\n",
    "    'link_type': 'U1',\n",
    "    'dim': 2,\n",
    "    'beta': 8.,\n",
    "    'num_samples': 4,\n",
    "    'num_steps': 5,\n",
    "    'eps': 0.2,\n",
    "    'loss_scale': 0.1,\n",
    "    'loss_eps': 1e-4,\n",
    "    'learning_rate_init': 1e-3,\n",
    "    'learning_rate_decay_steps': 100,\n",
    "    'learning_rate_decay_rate': 0.96,\n",
    "    'train_steps': 1000,\n",
    "    'record_loss_every': 50,\n",
    "    'data_steps': 1,\n",
    "    'save_steps': 50,\n",
    "    'print_steps': 5,\n",
    "    'logging_steps': 50,\n",
    "    'clip_value': 100,\n",
    "    'rand': False,\n",
    "    'metric': 'l2',\n",
    "}\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Using gauge_model.GaugeModel (compiled graph)\n",
    "\n",
    "Seems to have the same problem as before where the training time / step increases as training progresses. I think this is due to the graph constantly creating new `tf.reshape` operations each time a batch is fed through. Need to test further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-18T17:42:16.236438Z",
     "start_time": "2018-12-18T17:42:15.937568Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_conv = GaugeModel(params=params,\n",
    "                        conv_net=True,\n",
    "                        hmc=False,\n",
    "                        log_dir=None,\n",
    "                        restore=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-18T17:42:17.843497Z",
     "start_time": "2018-12-18T17:42:17.741300Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_conv.logging_steps = 50\n",
    "model_conv.data_steps = 1 \n",
    "model_conv.save_steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-18T19:10:10.235636Z",
     "start_time": "2018-12-18T17:43:09.740005Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_conv.train(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-18T08:39:13.979243Z",
     "start_time": "2018-12-18T08:31:27.667298Z"
    },
    "code_folding": [
     3,
     13
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tf.enable_resource_variables()\n",
    "tf.reset_default_graph()\n",
    "print(\"Building graph...\")\n",
    "model_conv = GaugeModelEager(params=params,\n",
    "                             conv_net=True,\n",
    "                             hmc=False,\n",
    "                             log_dir=None,\n",
    "                             restore=False,\n",
    "                             defun=False)\n",
    "x = tf.placeholder(tf.float32, shape=model_conv.samples.shape)\n",
    "samples_np = np.array(model_conv.lattice.samples, dtype=np.float32)\n",
    "#x = np.array(model_conv.lattice.samples, dtype=np.float32)\n",
    "loss, _, _ = gde.compute_loss(model_conv.dynamics, x, model_conv.params)\n",
    "train_op, loss, samples_out, accept_prob, _ = graph_step(\n",
    "    dynamics=model_conv.dynamics,\n",
    "    samples=x,\n",
    "    optimizer=model_conv.optimizer,\n",
    "    loss_fn=gde.compute_loss,\n",
    "    params=model_conv.params,\n",
    "    global_step=model_conv.global_step,\n",
    "    hmc=model_conv.hmc\n",
    ")\n",
    "session_conf = tf.ConfigProto()\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-18T08:42:12.211352Z",
     "start_time": "2018-12-18T08:40:38.935626Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#with tf.Session(config=session_conf) as sess:\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(\"Performing warmup...\")\n",
    "for _ in range(1):\n",
    "    _, _ = sess.run([train_op, loss], feed_dict={x: samples_np})\n",
    "print(\"done.\")\n",
    "print(\"Training...\")\n",
    "start_time = time.time()\n",
    "for i in range(10):\n",
    "    _, loss_np, samples_np, accept_prob_np = sess.run(\n",
    "        [train_op,\n",
    "         loss,\n",
    "         samples_out,\n",
    "         accept_prob],\n",
    "        feed_dict={x: samples_np}\n",
    "    )\n",
    "    print(f'step: {i}, loss: {loss_np}')\n",
    "#x_, v_, x_accept_prob, x_out = model_conv.dynamics.apply_transition(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     3,
     5
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "      with tf.Graph().as_default():\n",
    "        energy_fn, _, _ = l2hmc.get_scg_energy_fn()\n",
    "        x = tf.random_normal([hparams.n_samples, hparams.x_dim],\n",
    "                             dtype=tf.float32)\n",
    "        dynamics = l2hmc.Dynamics(\n",
    "            x_dim=hparams.x_dim,\n",
    "            minus_loglikelihood_fn=energy_fn,\n",
    "            n_steps=hparams.n_steps,\n",
    "            eps=hparams.eps)\n",
    "        loss, _, _ = l2hmc.compute_loss(dynamics, x)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=hparams.learning_rate)\n",
    "        train_op, loss, _ = graph_step(dynamics, optimizer, x)\n",
    "        # Single thread; fairer comparison against eager\n",
    "        session_conf = tf.ConfigProto(inter_op_parallelism_threads=1)\n",
    "        with tf.Session(config=session_conf) as sess:\n",
    "          sess.run(tf.global_variables_initializer())\n",
    "\n",
    "          # Warmup to reduce initialization effect when timing\n",
    "          for _ in range(hparams.n_warmup_iters):\n",
    "            _, _ = sess.run([train_op, loss])\n",
    "\n",
    "          # Training\n",
    "          start_time = time.time()\n",
    "          for i in range(hparams.n_iters):\n",
    "            _, loss_np = sess.run([train_op, loss])\n",
    "            print(\"Iteration %d: loss %.4f\" % (i, loss_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using tf.contrib.eager execution with 'defun' for compiling ops to graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### (Generic) HMC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T06:59:38.296664Z",
     "start_time": "2018-12-19T06:59:38.164626Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_hmc = GaugeModelEager(params=params,\n",
    "                            conv_net=False,\n",
    "                            hmc=True,\n",
    "                            log_dir=None,\n",
    "                            restore=False,\n",
    "                            defun=True)\n",
    "# 1m 22.9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T06:59:42.715898Z",
     "start_time": "2018-12-19T06:59:42.520793Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "observables_hmc = model_hmc.calc_observables(model_hmc.samples, \n",
    "                                             _print=True, \n",
    "                                             _write=True)\n",
    "total_actions, avg_plaquettes, top_charges = observables_hmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T07:25:30.689003Z",
     "start_time": "2018-12-19T06:59:50.520619Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_hmc.train(10, keep_samples=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T07:51:38.934424Z",
     "start_time": "2018-12-14T07:51:36.679499Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "steps = np.arange(len(model_hmc.data['average_plaquettes_arr']))\n",
    "_ = plot_run_data(model_hmc.data, model_hmc.params, \n",
    "                  steps, model_hmc.figs_dir, skip_steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ConvNet L2HMC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T23:34:09.157868Z",
     "start_time": "2018-12-20T23:34:09.090421Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T20:08:14.686281Z",
     "start_time": "2019-02-01T20:08:14.110663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory for new run: /Users/saforem2/ANL/l2hmc/gauge_logs_eager/run_2/\n",
      "time_size: 8\n",
      "\n",
      "space_size: 8\n",
      "\n",
      "link_type: U1\n",
      "\n",
      "dim: 2\n",
      "\n",
      "beta: 8.0\n",
      "\n",
      "num_samples: 4\n",
      "\n",
      "num_steps: 5\n",
      "\n",
      "eps: 0.2\n",
      "\n",
      "loss_scale: 0.1\n",
      "\n",
      "loss_eps: 0.0001\n",
      "\n",
      "learning_rate_init: 0.001\n",
      "\n",
      "learning_rate_decay_steps: 100\n",
      "\n",
      "learning_rate_decay_rate: 0.96\n",
      "\n",
      "train_steps: 1000\n",
      "\n",
      "record_loss_every: 50\n",
      "\n",
      "data_steps: 1\n",
      "\n",
      "save_steps: 50\n",
      "\n",
      "print_steps: 5\n",
      "\n",
      "logging_steps: 50\n",
      "\n",
      "clip_value: 100\n",
      "\n",
      "rand: False\n",
      "\n",
      "metric: l2\n",
      "\n",
      "total initialization time: 0.11693072319030762\n",
      "\n",
      "################################################################################\n",
      "Model parameters:\n",
      "log_dir: /Users/saforem2/ANL/l2hmc/gauge_logs_eager/run_2/\n",
      "\n",
      "info_dir: /Users/saforem2/ANL/l2hmc/gauge_logs_eager/run_2/run_info/\n",
      "\n",
      "figs_dir: /Users/saforem2/ANL/l2hmc/gauge_logs_eager/run_2/figures/\n",
      "\n",
      "_defun: True\n",
      "\n",
      "conv_net: True\n",
      "\n",
      "hmc: False\n",
      "\n",
      "time_size: 8\n",
      "\n",
      "space_size: 8\n",
      "\n",
      "link_type: U1\n",
      "\n",
      "dim: 2\n",
      "\n",
      "beta: 8.0\n",
      "\n",
      "num_samples: 4\n",
      "\n",
      "num_steps: 5\n",
      "\n",
      "eps: 0.2\n",
      "\n",
      "loss_scale: 0.1\n",
      "\n",
      "loss_eps: 0.0001\n",
      "\n",
      "learning_rate_init: 0.001\n",
      "\n",
      "learning_rate_decay_steps: 100\n",
      "\n",
      "learning_rate_decay_rate: 0.96\n",
      "\n",
      "train_steps: 1000\n",
      "\n",
      "record_loss_every: 50\n",
      "\n",
      "data_steps: 1\n",
      "\n",
      "save_steps: 50\n",
      "\n",
      "print_steps: 5\n",
      "\n",
      "logging_steps: 50\n",
      "\n",
      "clip_value: 100\n",
      "\n",
      "rand: False\n",
      "\n",
      "metric: l2\n",
      "\n",
      "batch_size: 4\n",
      "\n",
      "################################################################################\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_conv_defun = GaugeModelEager(params=params,\n",
    "                                   conv_net=True,\n",
    "                                   hmc=False,\n",
    "                                   log_dir=None,\n",
    "                                   restore=False,\n",
    "                                   defun=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T19:42:54.091443Z",
     "start_time": "2019-02-01T19:42:53.958286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory for new run: /Users/saforem2/ANL/l2hmc/gauge_logs_eager/run_1/\n",
      "time_size: 8\n",
      "\n",
      "space_size: 8\n",
      "\n",
      "link_type: U1\n",
      "\n",
      "dim: 2\n",
      "\n",
      "beta: 8.0\n",
      "\n",
      "num_samples: 4\n",
      "\n",
      "num_steps: 5\n",
      "\n",
      "eps: 0.2\n",
      "\n",
      "loss_scale: 0.1\n",
      "\n",
      "loss_eps: 0.0001\n",
      "\n",
      "learning_rate_init: 0.001\n",
      "\n",
      "learning_rate_decay_steps: 100\n",
      "\n",
      "learning_rate_decay_rate: 0.96\n",
      "\n",
      "train_steps: 1000\n",
      "\n",
      "record_loss_every: 50\n",
      "\n",
      "data_steps: 1\n",
      "\n",
      "save_steps: 50\n",
      "\n",
      "print_steps: 5\n",
      "\n",
      "logging_steps: 50\n",
      "\n",
      "clip_value: 100\n",
      "\n",
      "rand: False\n",
      "\n",
      "metric: l2\n",
      "\n",
      "total initialization time: 0.05965685844421387\n",
      "\n",
      "################################################################################\n",
      "Model parameters:\n",
      "log_dir: /Users/saforem2/ANL/l2hmc/gauge_logs_eager/run_1/\n",
      "\n",
      "info_dir: /Users/saforem2/ANL/l2hmc/gauge_logs_eager/run_1/run_info/\n",
      "\n",
      "figs_dir: /Users/saforem2/ANL/l2hmc/gauge_logs_eager/run_1/figures/\n",
      "\n",
      "_defun: False\n",
      "\n",
      "conv_net: True\n",
      "\n",
      "hmc: False\n",
      "\n",
      "time_size: 8\n",
      "\n",
      "space_size: 8\n",
      "\n",
      "link_type: U1\n",
      "\n",
      "dim: 2\n",
      "\n",
      "beta: 8.0\n",
      "\n",
      "num_samples: 4\n",
      "\n",
      "num_steps: 5\n",
      "\n",
      "eps: 0.2\n",
      "\n",
      "loss_scale: 0.1\n",
      "\n",
      "loss_eps: 0.0001\n",
      "\n",
      "learning_rate_init: 0.001\n",
      "\n",
      "learning_rate_decay_steps: 100\n",
      "\n",
      "learning_rate_decay_rate: 0.96\n",
      "\n",
      "train_steps: 1000\n",
      "\n",
      "record_loss_every: 50\n",
      "\n",
      "data_steps: 1\n",
      "\n",
      "save_steps: 50\n",
      "\n",
      "print_steps: 5\n",
      "\n",
      "logging_steps: 50\n",
      "\n",
      "clip_value: 100\n",
      "\n",
      "rand: False\n",
      "\n",
      "metric: l2\n",
      "\n",
      "batch_size: 4\n",
      "\n",
      "################################################################################\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#params['conv_net'] = True\n",
    "model_conv = GaugeModelEager(params=params,\n",
    "                             conv_net=True,\n",
    "                             hmc=False,\n",
    "                             log_dir=None,\n",
    "                             restore=False,\n",
    "                             defun=False)\n",
    "#model_conv._restore_model(model_conv.log_dir)\n",
    "#model_conv.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T19:47:50.032488Z",
     "start_time": "2019-02-01T19:47:49.677184Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "     STEP           LOSS       NORM. TIME     ACCEPT %        EPS           BETA           LR      \n",
      "----------------------------------------------------------------------------------------------------\n",
      "     0/1000          0             0             0            0.2           -1           0.001    \n"
     ]
    }
   ],
   "source": [
    "observables_conv = model_conv.calc_observables(model_conv.samples, update=True) \n",
    "helpers.print_run_data(model_conv.data, header=True)\n",
    "helpers.write_run_data(model_conv.files['run_info_file'], model_conv.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T19:50:31.700602Z",
     "start_time": "2019-02-01T19:48:22.580937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "     STEP           LOSS       NORM. TIME     ACCEPT %        EPS           BETA           LR      \n",
      "----------------------------------------------------------------------------------------------------\n",
      "     1/5           459.6         1.416       0.0003067       0.199          -1           0.001    \n",
      "     2/5           302.3         1.785        0.09863       0.1981          -1           0.001    \n",
      "     3/5            536          1.387       0.009224       0.1972          -1           0.001    \n",
      "     4/5           358.2         1.724       0.000661       0.1963          -1           0.001    \n",
      "Training complete.\n",
      "\n",
      "\n",
      "Saved checkpoint to: /Users/saforem2/ANL/l2hmc/gauge_logs_eager/run_1/ckpt-1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_conv.train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T22:56:39.849254Z",
     "start_time": "2019-02-01T20:08:29.230542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "     STEP           LOSS       NORM. TIME     ACCEPT %        EPS           BETA           LR      \n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-e35b843262de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_conv_defun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~/ANL/l2hmc/l2hmc/u1_model_eager.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, num_train_steps)\u001b[0m\n\u001b[0;32m    541\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m                 \u001b[0mhmc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhmc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 543\u001b[1;33m                 \u001b[0mdefun\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_defun\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    544\u001b[0m             )\n\u001b[0;32m    545\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~/ANL/l2hmc/l2hmc/u1_model_eager.py\u001b[0m in \u001b[0;36mtrain_one_iter\u001b[1;34m(dynamics, samples, optimizer, loss_fn, params, global_step, hmc, transition_fn, defun)\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[0mhmc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhmc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[0mtransition_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransition_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[0mdefun\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefun\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m     )\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~/ANL/l2hmc/l2hmc/l2hmc_eager/gauge_dynamics_eager.py\u001b[0m in \u001b[0;36mloss_and_grads\u001b[1;34m(dynamics, x, params, loss_fn, hmc, transition_fn, defun)\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m         _x, x_accept_prob, x_out = _get_new_sample(dynamics, x,\n\u001b[1;32m--> 529\u001b[1;33m                                                    transition_fn, defun)\n\u001b[0m\u001b[0;32m    530\u001b[0m         \u001b[1;31m#  _x, x_accept_prob, x_out = dynamics(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[0mloss_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdynamics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_accept_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefun\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1108\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1110\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_record\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_inputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_record\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcaptures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 660\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backprop_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_backprop_call\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    775\u001b[0m     \"\"\"\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backward_graph_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_construct_backprop_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m     \u001b[0mctx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_construct_backprop_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    751\u001b[0m     \u001b[0mbackwards_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstructured_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradients_wrt_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m     self._backward_graph_function = Function(\n\u001b[1;32m--> 753\u001b[1;33m         backwards_graph, attrs=backward_function_attr)\n\u001b[0m\u001b[0;32m    754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m     forward_function_attr = _parse_func_attrs({\n",
      "\u001b[1;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func_graph, attrs)\u001b[0m\n\u001b[0;32m    600\u001b[0m     self._inference_function = _EagerDefinedFunction(\n\u001b[0;32m    601\u001b[0m         \u001b[0m_inference_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 602\u001b[1;33m         self._func_graph.inputs, self._func_graph.outputs, self._attrs)\n\u001b[0m\u001b[0;32m    603\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backward_graph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, graph, inputs, outputs, attrs)\u001b[0m\n\u001b[0;32m    435\u001b[0m     \"\"\"\n\u001b[0;32m    436\u001b[0m     operations = [\n\u001b[1;32m--> 437\u001b[1;33m         \u001b[0mop\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mop\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_operations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mop\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     ]\n",
      "\u001b[1;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    436\u001b[0m     operations = [\n\u001b[0;32m    437\u001b[0m         \u001b[0mop\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mop\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_operations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 438\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mop\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m     ]\n\u001b[0;32m    440\u001b[0m     fn = pywrap_tensorflow.TF_GraphToFunction_wrapper(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_conv_defun.train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T07:09:25.429147Z",
     "start_time": "2018-12-21T03:19:05.105245Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "model_conv.train(500)\n",
    "# 3h 50m 20s with defun=False, including aux var z, 5 steps eps 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T08:09:47.269208Z",
     "start_time": "2018-12-21T08:09:47.191477Z"
    }
   },
   "outputs": [],
   "source": [
    "steps_arr = [0]\n",
    "steps_arr.extend(model_conv.steps_arr)\n",
    "\n",
    "_ = plot_run_data(model_conv.data, model_conv.params, steps_arr, \n",
    "                  model_conv.figs_dir, skip_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T08:10:25.250306Z",
     "start_time": "2018-12-21T08:10:21.120146Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = plot_run_data(model_conv.data, model_conv.params, steps_arr, \n",
    "                  model_conv.figs_dir, skip_steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FullyConnected (GenericNet) L2HMC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T19:27:58.786504Z",
     "start_time": "2018-12-21T19:27:58.654504Z"
    }
   },
   "outputs": [],
   "source": [
    "model_fc = GaugeModelEager(params=params,\n",
    "                           conv_net=False,\n",
    "                           hmc=False,\n",
    "                           log_dir=None,\n",
    "                           restore=False,\n",
    "                           defun=True)\n",
    "#model_fc.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T19:28:31.946838Z",
     "start_time": "2018-12-21T19:28:31.790074Z"
    }
   },
   "outputs": [],
   "source": [
    "observables_fc = model_fc.calc_observables(model_fc.samples, update=True) \n",
    "helpers.print_run_data(model_fc.data, header=True)\n",
    "helpers.write_run_data(model_fc.files['run_info_file'], model_fc.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-21T19:33:03.903Z"
    }
   },
   "outputs": [],
   "source": [
    "model_fc.train(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model (generate samples to measure Autocorrelation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Using CONV NET model: `model_conv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T08:12:25.172744Z",
     "start_time": "2018-12-21T08:12:25.055533Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#samples_conv = np.array(\n",
    "#    model_conv.lattice.samples.reshape((model_conv.batch_size, -1)),\n",
    "#    dtype=np.float32\n",
    "#)\n",
    "apply_transition_conv = tfe.defun(model_conv.dynamics.apply_transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T08:12:29.101256Z",
     "start_time": "2018-12-21T08:12:29.017383Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "samples_conv = tf.random_normal(shape=model_conv.samples.shape)\n",
    "samples_history_conv = []\n",
    "actions_history_conv = []\n",
    "avg_plaquettes_history_conv = []\n",
    "top_charges_history_conv = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T09:13:56.519543Z",
     "start_time": "2018-12-21T08:29:36.350474Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    #samples_history_conv.append(samples_conv.numpy())\n",
    "    t0 = time.time()\n",
    "    #_, _, _, samples_conv = apply_transition_conv(samples_conv)\n",
    "    _, _, _, samples_conv1 = model_conv1.dynamics.apply_transition(samples_conv1)\n",
    "    observables_conv = np.array(\n",
    "        model_conv.lattice.calc_plaq_observables(samples_conv1)\n",
    "    ).T\n",
    "    actions_history_conv.append(observables_conv[0])\n",
    "    avg_plaquettes_history_conv.append(observables_conv[1])\n",
    "    top_charges_history_conv.append(observables_conv[2])\n",
    "    step_time = (time.time() - t0) / (model_conv.num_steps * model_conv.batch_size)\n",
    "    print(f'step: {i}  time/step: {step_time:^6.4g} '\n",
    "          f'top_charge: {np.mean(observables_conv[2]):^6.4g} '\n",
    "          f'avg_plaq: {np.mean(observables_conv[1]):^6.4g}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T09:18:26.675929Z",
     "start_time": "2018-12-21T09:18:26.088931Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#samples_history_conv = np.array(samples_history_conv)\n",
    "actions_history_conv = np.array(actions_history_conv)\n",
    "avg_plaquettes_history_conv = np.array(avg_plaquettes_history_conv)\n",
    "top_charges_history_conv = np.array(top_charges_history_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T09:18:30.579284Z",
     "start_time": "2018-12-21T09:18:30.502084Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "top_charges_autocorr0_conv = autocorr(top_charges_history_conv[:, 0])\n",
    "top_charges_autocorr1_conv = autocorr(top_charges_history_conv[:, 1])\n",
    "\n",
    "top_charges_autocorr_conv = (top_charges_autocorr0_conv \n",
    "                            + top_charges_autocorr1_conv) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T09:20:11.350782Z",
     "start_time": "2018-12-21T09:20:11.243247Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_plaquettes_autocorr0_conv = autocorr(avg_plaquettes_history_conv[:, 0])\n",
    "avg_plaquettes_autocorr1_conv = autocorr(avg_plaquettes_history_conv[:, 1])\n",
    "\n",
    "avg_plaquettes_autocorr_conv = (avg_plaquettes_autocorr0_conv\n",
    "                                + avg_plaquettes_autocorr1_conv) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T09:22:32.275371Z",
     "start_time": "2018-12-21T09:22:29.545272Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "steps_conv = np.arange(len(top_charges_autocorr_conv))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "#ax.plot(steps_hmc, top_charges_autocorr_hmc, \n",
    "#        marker='', ls='-', label='topological_charge (HMC)')\n",
    "#ax.plot(steps_conv, avg_plaquettes_autocorr_conv,\n",
    "#        marker='', ls='--', label='avg plaquettes (L2HMC, ConvNet)')\n",
    "ax.plot(steps_conv, top_charges_autocorr_conv,\n",
    "        marker='', ls='-', label='topological charges (L2HMC, ConvNet)')\n",
    "#ax.plot(steps1, top_charges_autocorr, marker='', ls='-', label='topological_charge (L2HMC)')\n",
    "ax.set_ylabel('Autocorrelation', fontsize=14)\n",
    "ax.set_xlabel('Gradient Evaluations', fontsize=14)\n",
    "ax.set_title(\"\")\n",
    "ax.legend(loc='best', fontsize=12)\n",
    "fig.savefig(os.path.join(model_conv.figs_dir, \n",
    "                         'top_charge_autocorrelation_fn_hmc.pdf'), \n",
    "            dpi=400, bbox_inches='tight')\n",
    "#ax.set_xlim((ax.get_xlim()[0], 1000))\n",
    "#ax.set_xlim((-5, 1000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Using HMC model: `model_hmc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T07:52:05.710139Z",
     "start_time": "2018-12-14T07:52:05.582619Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "samples_hmc = np.array(\n",
    "    model_hmc.lattice.samples.reshape((model_hmc.batch_size, -1)),\n",
    "    dtype=np.float32\n",
    ")\n",
    "apply_transition_hmc = tfe.defun(model_hmc.dynamics.apply_transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T07:52:10.013636Z",
     "start_time": "2018-12-14T07:52:09.898056Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "samples_hmc = tf.random_normal(shape=model_hmc.samples.shape)\n",
    "samples_history_hmc = []\n",
    "actions_history_hmc = []\n",
    "avg_plaquettes_history_hmc = []\n",
    "top_charges_history_hmc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T08:28:52.362562Z",
     "start_time": "2018-12-14T07:52:33.967852Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in range(5000):\n",
    "    samples_history_hmc.append(samples_hmc.numpy())\n",
    "    t0 = time.time()\n",
    "    _, _, _, samples_hmc = apply_transition_hmc(samples_hmc)\n",
    "    observables_hmc = np.array(\n",
    "        model_hmc.lattice.calc_plaq_observables(samples_hmc)\n",
    "    ).T\n",
    "    actions_history_hmc.append(observables_hmc[0])\n",
    "    avg_plaquettes_history_hmc.append(observables_hmc[1])\n",
    "    top_charges_history_hmc.append(observables_hmc[2])\n",
    "    print(f'step: {i}  time/step: {time.time() - t0:^6.4g}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T08:30:26.304591Z",
     "start_time": "2018-12-14T08:30:26.135550Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "samples_history_hmc = np.array(samples_history_hmc)\n",
    "actions_history_hmc = np.array(actions_history_hmc)\n",
    "avg_plaquettes_history_hmc = np.array(avg_plaquettes_history_hmc)\n",
    "top_charges_history_hmc = np.array(top_charges_history_hmc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T08:31:28.728067Z",
     "start_time": "2018-12-14T08:31:28.585706Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "top_charges_autocorr0_hmc = autocorr(top_charges_history_hmc[:, 0])\n",
    "top_charges_autocorr1_hmc = autocorr(top_charges_history_hmc[:, 1])\n",
    "\n",
    "top_charges_autocorr_hmc = (top_charges_autocorr0_hmc \n",
    "                            + top_charges_autocorr1_hmc) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Compute Autocorrelation spectrum directly from samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T01:40:56.408185Z",
     "start_time": "2018-12-14T01:34:16.914581Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "samples_ac_spectrum_hmc = compute_ac_spectrum(samples_history_hmc, \n",
    "                                              samples_history_mean_hmc)\n",
    "\n",
    "steps_hmc = np.arange(len(samples_ac_spectrum_hmc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T02:17:32.367700Z",
     "start_time": "2018-12-14T02:17:32.266397Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "samples_ac_spectrum_hmc_file = os.path.join(model_hmc.info_dir,\n",
    "                                            'samples_ac_spectrum_hmc.pkl')\n",
    "top_charges_acl_spectrum_hmc_file = os.path.join(model_hmc.info_dir,\n",
    "                                                 'top_charges_acl_spectrum_hmc.pkl')\n",
    "actions_acl_spectrum_hmc_file = os.path.join(model_hmc.info_dir,\n",
    "                                             'actions_acl_spectrum_hmc.pkl')\n",
    "avg_plaquette_acl_spectrum_hmc_file = os.path.join(model_hmc.info_dir,\n",
    "                                                   'avg_plaquette_acl_spectrum_hmc.pkl')\n",
    "with open(samples_ac_spectrum_hmc_file, 'wb') as f:\n",
    "    pickle.dump(samples_ac_spectrum_hmc, f)\n",
    "\n",
    "with open(top_charges_acl_spectrum_hmc_file, 'wb') as f:\n",
    "    pickle.dump(top_charges_acl_spectrum_hmc, f)\n",
    "    \n",
    "with open(actions_acl_spectrum_hmc_file, 'wb') as f:\n",
    "    pickle.dump(actions_acl_spectrum_hmc, f)\n",
    "    \n",
    "with open(avg_plaquette_acl_spectrum_hmc_file, 'wb') as f:\n",
    "    pickle.dump(avg_plaquette_acl_spectrum_hmc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T01:41:00.602846Z",
     "start_time": "2018-12-14T01:41:00.528289Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "steps = np.arange(len(samples_ac_spectrum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T01:41:06.075950Z",
     "start_time": "2018-12-14T01:41:04.811314Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(steps_hmc, samples_ac_spectrum_hmc, marker='', ls='-', label='HMC')\n",
    "ax.plot(steps, samples_ac_spectrum, marker='', ls='-', label='L2HMC')\n",
    "ax.set_ylabel('Autocorrelation', fontsize=14)\n",
    "ax.set_xlabel('Gradient Evaluations', fontsize=14)\n",
    "ax.set_title(\"Autocorrelation directly from samples (Eq. 15)\", fontsize=16)\n",
    "ax.legend(loc='best')\n",
    "fig.savefig(os.path.join(model_hmc.figs_dir, 'samples_autocorrelation_fn_hmc.pdf'), \n",
    "            dpi=400, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Compute Autocorrelation for Topological charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T01:52:53.413309Z",
     "start_time": "2018-12-14T01:41:11.188528Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#top_charges_acl_spectrum_hmc = compute_autocorrelation(top_charges_history_hmc, \n",
    "#                                                       top_charges_mean_hmc,\n",
    "#                                                       top_charges_var_hmc)\n",
    "#actions_acl_spectrum_hmc = compute_autocorrelation(actions_history_hmc, \n",
    "#                                                   actions_mean_hmc,\n",
    "#                                                   actions_var_hmc)\n",
    "#avg_plaquette_acl_spectrum_hmc = compute_autocorrelation(avg_plaquettes_history_hmc,\n",
    "#                                                         avg_plaquettes_mean_hmc,\n",
    "#                                                         avg_plaquettes_var_hmc)\n",
    "#steps_hmc = np.arange(len(top_charges_acl_spectrum_hmc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T03:00:07.810848Z",
     "start_time": "2018-12-14T02:55:33.227061Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#top_charges_acl_spectrum_hmc = compute_autocorrelation(top_charges_history_hmc, \n",
    "#                                                       top_charges_mean_hmc,\n",
    "#                                                       top_charges_var_hmc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T09:31:05.904999Z",
     "start_time": "2018-12-14T09:31:05.780532Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "top_charges_autocorr0 = autocorr(top_charges_history[:, 0])\n",
    "top_charges_autocorr1 = autocorr(top_charges_history[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T09:33:15.882847Z",
     "start_time": "2018-12-14T09:33:15.767267Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#top_charges_autocorr = (top_charges_autocorr0 + top_charges_autocorr1) / 2\n",
    "#avg_plaquette_autocorr0 = autocorr(avg_plaquettes_history[:, 0])\n",
    "#avg_plaquette_autocorr1 = autocorr(avg_plaquettes_history[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T08:31:51.598658Z",
     "start_time": "2018-12-14T08:31:51.476146Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_plaquette_autocorr0_hmc = autocorr(avg_plaquettes_history_hmc[:, 0])\n",
    "avg_plaquette_autocorr1_hmc = autocorr(avg_plaquettes_history_hmc[:, 1])\n",
    "\n",
    "#avg_plaquette_autocorr = (avg_plaquette_autocorr0\n",
    "#                          + avg_plaquette_autocorr1) / 2\n",
    "\n",
    "avg_plaquette_autocorr_hmc = (avg_plaquette_autocorr0_hmc\n",
    "                              + avg_plaquette_autocorr1_hmc) / 2\n",
    "\n",
    "#steps = np.arange(len(avg_plaquette_autocorr))\n",
    "steps_hmc = np.arange(len(avg_plaquette_autocorr_hmc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T16:40:53.346644Z",
     "start_time": "2018-12-14T16:40:52.902055Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(steps_hmc, avg_plaquette_autocorr_hmc, marker='', ls='-', label='Average plaquette (HMC)')\n",
    "ax.plot(steps_conv, avg_plaquette_autocorr_conv, marker='', ls='-', label='Average plaquette (L2HMC, ConvNet)')\n",
    "#ax.plot(steps, avg_plaquette_autocorr, marker='', ls='-', label='Average plaquette (L2HMC)')\n",
    "#ax.plot(steps, avg_plaquette_autocorr1, marker='', ls='-', label='Average plaquette1 (L2HMC)')\n",
    "ax.set_ylabel('Autocorrelation', fontsize=14)\n",
    "ax.set_xlabel('Gradient Evaluations', fontsize=14)\n",
    "ax.legend(loc='best', fontsize=12)\n",
    "#fig.savefig(os.path.join(model_hmc.figs_dir, 'avg_plaquette_autocorrelation_fn_hmc.pdf'), \n",
    "#            dpi=400, bbox_inches='tight')\n",
    "#ax.set_xlim((-20, 450))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T09:35:04.331319Z",
     "start_time": "2018-12-14T09:35:04.111108Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T16:38:13.666800Z",
     "start_time": "2018-12-14T16:38:13.236315Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "steps1 = np.arange(len(top_charges_autocorr0))\n",
    "steps_hmc = np.arange(len(top_charges_autocorr0_hmc))\n",
    "steps_conv = np.arange(len(top_charges_autocorr_conv))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(steps_hmc, top_charges_autocorr_hmc, \n",
    "        marker='', ls='-', label='topological_charge (HMC)')\n",
    "ax.plot(steps_conv, top_charges_autocorr_conv,\n",
    "        marker='', ls='-', label='topological_charges (L2HMC, ConvNet)')\n",
    "#ax.plot(steps1, top_charges_autocorr, marker='', ls='-', label='topological_charge (L2HMC)')\n",
    "ax.set_ylabel('Autocorrelation', fontsize=14)\n",
    "ax.set_xlabel('Gradient Evaluations', fontsize=14)\n",
    "ax.set_title(\"\")\n",
    "ax.legend(loc='best', fontsize=12)\n",
    "fig.savefig(os.path.join(model_hmc.figs_dir, \n",
    "                         'top_charge_autocorrelation_fn_hmc.pdf'), \n",
    "            dpi=400, bbox_inches='tight')\n",
    "#ax.set_xlim((-20, 500))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T02:10:39.698274Z",
     "start_time": "2018-12-14T02:10:37.168685Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "#ax.semilogy(steps_hmc, actions_acl_spectrum_hmc, marker='', ls='-', label='total action (HMC)')\n",
    "ax.semilogy(steps_hmc, avg_plaquette_acl_spectrum_hmc, marker='', ls='-', label='average plaquette (HMC)')\n",
    "#ax.plot(steps, actions_acl_spectrum, marker='', ls='-', label='total action (L2HMC)')\n",
    "ax.semilogy(steps, avg_plaquette_acl_spectrum, marker='', ls='-', label='average plaquette (L2HMC)')\n",
    "ax.set_ylabel('Autocorrelation', fontsize=14)\n",
    "ax.set_xlabel('Gradient Evaluations', fontsize=14)\n",
    "ax.set_title(\"\")\n",
    "ax.legend(loc='best', fontsize=12)\n",
    "fig.savefig(os.path.join(model_hmc.figs_dir, 'avg_plaquette_autocorrelation_fn_hmc_semilogy.pdf'), \n",
    "            dpi=400, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Using GENERIC NET (Fully-Connected) model: `model_fc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T00:23:32.915359Z",
     "start_time": "2018-12-19T00:23:32.729199Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "apply_transition_fc = tfe.defun(model_fc.dynamics.apply_transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T00:28:37.716470Z",
     "start_time": "2018-12-19T00:28:37.628579Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "  #samples_history = []\n",
    "  #for i in range(eval_iters):\n",
    "    #samples_history.append(samples.numpy())\n",
    "    #_, _, _, samples = apply_transition(samples)\n",
    "  #samples_history = np.array(samples_history)\n",
    "  #print(\"Sampling complete.\")\n",
    "  #sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T00:33:20.243944Z",
     "start_time": "2018-12-19T00:33:20.172795Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#samples_fc = tf.random_normal(shape=model_fc.samples.shape)\n",
    "samples_fc = model_fc.samples\n",
    "samples_history_fc = []\n",
    "actions_history_fc = []\n",
    "avg_plaquettes_history_fc = []\n",
    "top_charges_history_fc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T00:41:06.097854Z",
     "start_time": "2018-12-19T00:33:49.546615Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_, _, _, _samples_fc = apply_transition_fc(samples_fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T01:02:05.023524Z",
     "start_time": "2018-12-19T01:02:04.724839Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_observables_fc = model_fc.lattice.calc_plaq_observables(_samples_fc.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T01:02:11.581853Z",
     "start_time": "2018-12-19T01:02:11.504467Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_observables_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T01:05:59.379417Z",
     "start_time": "2018-12-19T01:02:45.029871Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    samples_history_fc.append(samples_fc.numpy())\n",
    "    t0 = time.time()\n",
    "    _, _, _, samples_fc = apply_transition_fc(samples_fc)\n",
    "    observables_fc = np.array(\n",
    "        model_fc.lattice.calc_plaq_observables(samples_fc.numpy())\n",
    "    ).T\n",
    "    actions_history_fc.append(observables_fc[0])\n",
    "    avg_plaquettes_history_fc.append(observables_fc[1])\n",
    "    top_charges_history_fc.append(observables_fc[2])\n",
    "    step_time = (time.time() - t0) / (model_fc.num_steps * model_fc.batch_size)\n",
    "    print(f'step: {i}  time/step: {step_time:^6.4g} '\n",
    "          f'top_charge: {np.mean(observables_conv[2]):^6.4g} '\n",
    "          f'avg_plaq: {np.mean(observables_conv[1]):^6.4g}')\n",
    "    #print(f'step: {i}  time/step: {step_time:^6.4g}  avg_plaq: {np.mean(observables_fc[1]):^6.4g}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T01:07:05.357581Z",
     "start_time": "2018-12-19T01:07:05.275828Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "samples_history_fc = np.array(samples_history_fc)\n",
    "actions_history_fc = np.array(actions_history_fc)\n",
    "avg_plaquettes_history_fc = np.array(avg_plaquettes_history_fc)\n",
    "top_charges_history_fc = np.array(top_charges_history_fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T01:07:07.553358Z",
     "start_time": "2018-12-19T01:07:07.456296Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "top_charges_autocorr0_fc = autocorr(top_charges_history_fc[:, 0])\n",
    "top_charges_autocorr1_fc = autocorr(top_charges_history_fc[:, 1])\n",
    "\n",
    "top_charges_autocorr_fc = (top_charges_autocorr0_fc \n",
    "                            + top_charges_autocorr1_fc) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T01:11:03.992337Z",
     "start_time": "2018-12-19T01:11:03.926015Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_plaquettes_autocorr0_fc = autocorr(avg_plaquettes_history_fc[:, 0])\n",
    "avg_plaquettes_autocorr1_fc = autocorr(avg_plaquettes_history_fc[:, 1])\n",
    "\n",
    "avg_plaquettes_autocorr_fc = (avg_plaquettes_autocorr0_fc\n",
    "                              + avg_plaquettes_autocorr1_fc) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Topological Charge Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T01:10:14.471094Z",
     "start_time": "2018-12-19T01:10:13.806675Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "steps_fc = np.arange(len(top_charges_autocorr_fc))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "#ax.plot(steps_hmc, top_charges_autocorr_hmc, \n",
    "#        marker='', ls='-', label='topological_charge (HMC)')\n",
    "ax.plot(steps_fc, top_charges_autocorr_fc,\n",
    "        marker='', ls='-', label='topological_charges (L2HMC, FCNet)')\n",
    "#ax.plot(steps1, top_charges_autocorr, marker='', ls='-', label='topological_charge (L2HMC)')\n",
    "ax.set_ylabel('Autocorrelation', fontsize=14)\n",
    "ax.set_xlabel('Gradient Evaluations', fontsize=14)\n",
    "ax.set_title(\"\")\n",
    "ax.legend(loc='best', fontsize=12)\n",
    "fig.savefig(os.path.join(model_fc.figs_dir, \n",
    "                         'top_charge_autocorrelation_fn_hmc.pdf'), \n",
    "            dpi=400, bbox_inches='tight')\n",
    "#ax.set_xlim((-20, 500))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Average Plaquette Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T01:13:51.800013Z",
     "start_time": "2018-12-19T01:13:50.955270Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "steps_fc = np.arange(len(avg_plaquettes_autocorr_fc))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "#ax.plot(steps_hmc, avg_plaquettes_autocorr_hmc, \n",
    "#        marker='', ls='-', label='topological_charge (HMC)')\n",
    "ax.plot(steps_fc, avg_plaquettes_autocorr_fc,\n",
    "        marker='', ls='-', label='Average Plaquettes (L2HMC, FCNet)')\n",
    "#ax.plot(steps1, avg_plaquettes_autocorr, marker='', ls='-', label='topological_charge (L2HMC)')\n",
    "ax.set_ylabel('Autocorrelation', fontsize=14)\n",
    "ax.set_xlabel('Gradient Evaluations', fontsize=14)\n",
    "ax.set_title(\"\")\n",
    "ax.legend(loc='best', fontsize=12)\n",
    "fig.savefig(os.path.join(model_fc.figs_dir, \n",
    "                         'avg_plaquettes_autocorrelation_fn_hmc.pdf'), \n",
    "            dpi=400, bbox_inches='tight')\n",
    "#ax.set_xlim((-20, 500))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "apply_transition_conv = tfe.defun(model_conv.dynamics.apply_transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T08:44:05.385477Z",
     "start_time": "2018-12-14T08:44:05.286032Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "samples_fc = np.array(model.lattice.samples.reshape((model.batch_size, -1)), dtype=np.float32)\n",
    "print(samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T08:44:13.259434Z",
     "start_time": "2018-12-14T08:44:13.144959Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "apply_transition = tfe.defun(model.dynamics.apply_transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T09:26:23.904041Z",
     "start_time": "2018-12-14T08:44:15.032612Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "samples = tf.random_normal(shape=model.samples.shape)\n",
    "samples_history = []\n",
    "actions_history = []\n",
    "avg_plaquettes_history = []\n",
    "top_charges_history = []\n",
    "for i in range(5000):\n",
    "    samples_history.append(samples.numpy())\n",
    "    t0 = time.time()\n",
    "    _, _, _, samples = apply_transition(samples)\n",
    "    observables = np.array(model.lattice.calc_plaq_observables(samples)).T\n",
    "    actions_history.append(observables[0])\n",
    "    avg_plaquettes_history.append(observables[1])\n",
    "    top_charges_history.append(observables[2])\n",
    "    print(f'step: {i}  time/step: {time.time() - t0:^6.4g}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T09:30:56.834401Z",
     "start_time": "2018-12-14T09:30:56.674516Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "samples_history = np.array(samples_history)\n",
    "actions_history = np.array(actions_history)\n",
    "avg_plaquettes_history = np.array(avg_plaquettes_history)\n",
    "top_charges_history = np.array(top_charges_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T16:37:06.454184Z",
     "start_time": "2018-12-14T16:37:06.289867Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "top_charges_autocorr0_conv = autocorr(top_charges_history_conv[:, 0])\n",
    "top_charges_autocorr1_conv = autocorr(top_charges_history_conv[:, 1])\n",
    "avg_plaquette_autocorr0_conv = autocorr(avg_plaquettes_history_conv[:, 0])\n",
    "avg_plaquette_autocorr1_conv = autocorr(avg_plaquettes_history_conv[:, 1])\n",
    "\n",
    "top_charges_autocorr_conv = (top_charges_autocorr0_conv \n",
    "                             + top_charges_autocorr1_conv) / 2\n",
    "avg_plaquette_autocorr_conv = (avg_plaquette_autocorr0_conv\n",
    "                               + avg_plaquette_autocorr1_conv) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Compute Autocorrelation spectrum directly from samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T01:21:52.745496Z",
     "start_time": "2018-12-14T01:15:15.161556Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "samples_ac_spectrum = compute_ac_spectrum(samples_history, \n",
    "                                          samples_history_mean,\n",
    "                                          samples_history_var)\n",
    "\n",
    "#ax.plot(steps, samples_ac_spectrum, marker='', ls='-', label='L2HMC')\n",
    "steps = np.arange(len(samples_ac_spectrum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T22:43:11.776392Z",
     "start_time": "2018-12-13T22:43:11.349847Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T22:44:41.448991Z",
     "start_time": "2018-12-13T22:44:40.981327Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(steps, samples_ac_spectrum, marker='', ls='-', label='L2HMC')\n",
    "ax.set_ylabel('Autocorrelation', fontsize=14)\n",
    "ax.set_xlabel('Gradient Evaluations', fontsize=14)\n",
    "ax.set_title(\"Autocorrelation directly from samples (Eq. 15)\", fontsize=16)\n",
    "ax.legend(loc='best')\n",
    "fig.savefig(os.path.join(model.figs_dir, 'samples_autocorrelation_fn.pdf'), \n",
    "            dpi=400, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T22:49:43.051595Z",
     "start_time": "2018-12-13T22:49:42.954649Z"
    },
    "hidden": true
   },
   "source": [
    "##### Compute Autocorrelation for Topological charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T01:34:12.578437Z",
     "start_time": "2018-12-14T01:21:58.399213Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "top_charges_acl_spectrum = compute_autocorrelation(top_charges_history, \n",
    "                                                   top_charges_mean,\n",
    "                                                   top_charges_var)\n",
    "actions_acl_spectrum = compute_autocorrelation(actions_history, \n",
    "                                               actions_mean,\n",
    "                                               actions_var)\n",
    "avg_plaquette_acl_spectrum = compute_autocorrelation(avg_plaquettes_history,\n",
    "                                                     avg_plaquettes_mean,\n",
    "                                                     avg_plaquettes_var)\n",
    "steps = np.arange(len(top_charges_acl_spectrum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T22:45:51.611713Z",
     "start_time": "2018-12-13T22:45:50.949613Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(steps, top_charges_acl_spectrum, marker='', ls='-', label='topological_charge (L2HMC)')\n",
    "ax.plot(steps, actions_acl_spectrum, marker='', ls='-', label='total action (L2HMC)')\n",
    "ax.plot(steps, avg_plaquette_acl_spectrum, marker='', ls='-', label='average plaquette (L2HMC)')\n",
    "ax.set_ylabel('Autocorrelation', fontsize=14)\n",
    "ax.set_xlabel('Gradient Evaluations', fontsize=14)\n",
    "ax.set_title(\"\")\n",
    "ax.legend(loc='best', fontsize=12)\n",
    "fig.savefig(os.path.join(model.figs_dir, 'observables_autocorrelation_fn.pdf'), \n",
    "            dpi=400, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T02:19:31.466750Z",
     "start_time": "2018-12-14T02:19:31.376192Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "samples_ac_spectrum_file = os.path.join(model.info_dir,\n",
    "                                        'samples_ac_spectrum.pkl')\n",
    "top_charges_acl_spectrum_file = os.path.join(model.info_dir,\n",
    "                                             'top_charges_acl_spectrum.pkl')\n",
    "actions_acl_spectrum_file = os.path.join(model.info_dir,\n",
    "                                         'actions_acl_spectrum.pkl')\n",
    "avg_plaquette_acl_spectrum_file = os.path.join(model.info_dir,\n",
    "                                               'avg_plaquette_acl_spectrum.pkl')\n",
    "with open(samples_ac_spectrum_file, 'wb') as f:\n",
    "    pickle.dump(samples_ac_spectrum, f)\n",
    "\n",
    "with open(top_charges_acl_spectrum_file, 'wb') as f:\n",
    "    pickle.dump(top_charges_acl_spectrum, f)\n",
    "    \n",
    "with open(actions_acl_spectrum_file, 'wb') as f:\n",
    "    pickle.dump(actions_acl_spectrum, f)\n",
    "    \n",
    "with open(avg_plaquette_acl_spectrum_file, 'wb') as f:\n",
    "    pickle.dump(avg_plaquette_acl_spectrum, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load / Restore from previous run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T04:00:33.091598Z",
     "start_time": "2018-12-13T04:00:32.668846Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_l2hmc = GaugeModelEager(params=params, log_dir='../../gauge_logs_eager/run_40/', restore=True, use_defun=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T04:01:43.586141Z",
     "start_time": "2018-12-13T04:01:43.480457Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_plaq_arr_hmc = model.data['average_plaquettes_arr']\n",
    "top_charge_arr_hmc = model.data['topological_charges_arr']\n",
    "total_actions_arr_hmc = model.data['total_actions_arr']\n",
    "steps = np.arange(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T04:03:13.031213Z",
     "start_time": "2018-12-13T04:03:12.934637Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_plaq_arr_hmc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T04:03:19.506433Z",
     "start_time": "2018-12-13T04:03:19.414913Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_plaq_arr_l2hmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T08:18:03.674997Z",
     "start_time": "2018-12-13T08:18:03.563603Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "steps = np.arange(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Compare plots across runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T08:18:11.516889Z",
     "start_time": "2018-12-13T08:18:05.034071Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_run_data(model.data, model.params, steps, model.figs_dir, skip_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T04:20:14.891410Z",
     "start_time": "2018-12-13T04:20:14.712777Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "average_plaquettes_file_l2hmc = os.path.join(model_l2hmc.info_dir,\n",
    "                                             'average_plaquettes.npy')\n",
    "top_charge_file_l2hmc = os.path.join(model_l2hmc.info_dir,\n",
    "                                     'topological_charges.npy')\n",
    "total_actions_file_l2hmc = os.path.join(model_l2hmc.info_dir,\n",
    "                                        'total_actions.npy')\n",
    "avg_plaq_arr_l2hmc = np.load(average_plaquettes_file_l2hmc)\n",
    "top_charge_arr_l2hmc = np.load(top_charge_file_l2hmc)\n",
    "total_actions_arr_l2hmc = np.load(total_actions_file_l2hmc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T04:23:04.178173Z",
     "start_time": "2018-12-13T04:23:04.076313Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_l2hmc.topological_charges_arr = []\n",
    "model_l2hmc.average_plaquettes_arr = []\n",
    "model_l2hmc.total_actions_arr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T04:23:07.260973Z",
     "start_time": "2018-12-13T04:23:07.181720Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_l2hmc._update_data(total_actions_arr_l2hmc,\n",
    "                         avg_plaq_arr_l2hmc,\n",
    "                         top_charge_arr_l2hmc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T04:28:18.185406Z",
     "start_time": "2018-12-13T04:28:17.972463Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(steps, avg_plaq_arr_hmc[:, 0], marker='', markersize=4., ls='--', label=f'Generic HMC (sample 0) (avg: {np.mean(avg_plaq_arr_hmc[:, 0]):^5.4g})')\n",
    "ax.plot(steps, avg_plaq_arr_hmc[:, 1], marker='', markersize=4., ls='--', label=f'Generic HMC (sample 1) (avg: {np.mean(avg_plaq_arr_hmc[:, 1]):^5.4g})')\n",
    "ax.plot(steps, avg_plaq_arr_l2hmc[:, 0], marker='', markersize=4., ls='-', label=f'L2HMC (sample 0) (avg: {np.mean(avg_plaq_arr_l2hmc[:, 0]):^5.4g})')\n",
    "ax.plot(steps, avg_plaq_arr_l2hmc[:, 1], marker='', markersize=4., ls='-', label=f'L2HMC (sample 1) (avg: {np.mean(avg_plaq_arr_l2hmc[:, 1]):^5.4g})')\n",
    "#ax.plot(steps, avg_plaq_arr_l2hmc[:, 0], marker='.', ls='-', label=f'L2HMC (avg: {np.mean(avg_plaq_arr[:, 1]):^5.4g})')\n",
    "#ax.axhline(np.mean(avg_plaq_arr[:,0]), xmin=0, xmax=500, color='C0', ls='-', lw=2., label='Sample 1 (avg)')\n",
    "#ax.axhline(np.mean(avg_plaq_arr[:,1]), xmin=0, xmax=500, color='C1', ls='-', lw=2., label='Sample 2 (avg)')\n",
    "ax.axhline(u1_plaq_exact(params['beta']), xmin=0, xmax=500, color='k', lw=2., label=f\"Exact ({u1_plaq_exact(params['beta']):^5.4g})\")\n",
    "ax.set_ylabel('Avg. Plaquette')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_xlim((0, 20))\n",
    "ax.legend(loc='best')\n",
    "#file_name = os.path.join(model.figs_dir, 'average_plaquettes.pdf')\n",
    "#fig.savefig(file_name, dpi=400, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T04:32:35.125520Z",
     "start_time": "2018-12-13T04:32:34.954990Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "#ax.plot(steps[::5], top_charge_arr[:, 0][::5], marker='.', ls='-', label='sample 1')\n",
    "#ax.plot(steps[::5], top_charge_arr[:, 1][::5], marker='.', ls='-', label='sample 2')\n",
    "_ = ax.plot(steps, top_charge_arr_hmc[:, 0], marker='', markersize=4., ls='--', label=f'Generic HMC (sample 0) (avg: {np.mean(top_charge_arr_hmc[:, 0]):^5.4g})')\n",
    "_ = ax.plot(steps, top_charge_arr_hmc[:, 1], marker='', markersize=4., ls='--', label=f'Generic HMC (sample 1) (avg: {np.mean(top_charge_arr_hmc[:, 1]):^5.4g})')\n",
    "_ = ax.plot(steps, top_charge_arr_l2hmc[:, 0], marker='', markersize=4., ls='-', label=f'L2HMC (sample 0) (avg: {np.mean(top_charge_arr_l2hmc[:, 0]):^5.4g})')\n",
    "_ = ax.plot(steps, top_charge_arr_l2hmc[:, 1], marker='', markersize=4., ls='-', label=f'L2HMC (sample 1) (avg: {np.mean(top_charge_arr_l2hmc[:, 1]):^5.4g})')\n",
    "#ax.axhline(u1_plaq_exact(params['beta']), xmin=0, xmax=500, color='r', label='Exact')\n",
    "_ = ax.set_ylabel('Topological Charge')\n",
    "_ = ax.set_xlabel('Step')\n",
    "_ = ax.legend(loc='best')\n",
    "_ = ax.set_xlim((0, 25))\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T04:33:06.474008Z",
     "start_time": "2018-12-13T04:33:06.317131Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "#ax.plot(steps[::5], total_action_arr[:, 0][::5], marker='.', ls='-', label='sample 1')\n",
    "#ax.plot(steps[::5], total_action_arr[:, 1][::5], marker='.', ls='-', label='sample 2')\n",
    "_ = ax.plot(steps, total_action_arr_hmc[:, 0], marker='', markersize=4., ls='--', label=f'Generic HMC (sample 0) (avg: {np.mean(total_action_arr_hmc[:, 0]):^5.4g})')\n",
    "_ = ax.plot(steps, total_action_arr_hmc[:, 1], marker='', markersize=4., ls='--', label=f'Generic HMC (sample 1) (avg: {np.mean(total_action_arr_hmc[:, 1]):^5.4g})')\n",
    "_ = ax.plot(steps, total_actions_arr_l2hmc[:, 0], marker='', markersize=4., ls='-', label=f'L2HMC (sample 0) (avg: {np.mean(total_actions_arr_l2hmc[:, 0]):^5.4g})')\n",
    "_ = ax.plot(steps, total_actions_arr_l2hmc[:, 1], marker='', markersize=4., ls='-', label=f'L2HMC (sample 1) (avg: {np.mean(total_actions_arr_l2hmc[:, 1]):^5.4g})')\n",
    "#ax.axhline(u1_plaq_exact(params['beta']), xmin=0, xmax=500, color='r', label='Exact')\n",
    "_ = ax.set_ylabel('Total Action')\n",
    "_ = ax.set_xlabel('Step')\n",
    "_ = ax.legend(loc='best')\n",
    "_ = ax.set_xlim((0, 25))\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T00:14:19.151871Z",
     "start_time": "2018-12-13T00:14:19.035598Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_plaq1_mean, avg_plaq1_err = calc_avg_vals_errors(avg_plaq_arr[100:, 0], num_blocks=100)\n",
    "avg_plaq2_mean, avg_plaq2_err = calc_avg_vals_errors(avg_plaq_arr[100:, 1], num_blocks=100)\n",
    "print(avg_plaq1_mean, avg_plaq1_err)\n",
    "print(avg_plaq2_mean, avg_plaq2_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T00:13:42.986603Z",
     "start_time": "2018-12-13T00:13:42.901028Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "u1_plaq_exact(params['beta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T23:07:35.529041Z",
     "start_time": "2018-12-12T23:07:35.265638Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(model.learning_rate.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T23:06:23.430211Z",
     "start_time": "2018-12-12T23:06:23.082303Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.data['avg_top_charge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T12:04:57.100137Z",
     "start_time": "2018-12-09T12:04:56.403756Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_plaqs = np.array(model.average_plaquettes_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T12:04:59.002828Z",
     "start_time": "2018-12-09T12:04:58.884010Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_plaqs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T05:07:42.011099Z",
     "start_time": "2018-12-09T05:07:41.925679Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "u1_plaq_exact(params['beta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Build tf.keras.Model using tf.data.Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T03:17:39.555556Z",
     "start_time": "2018-12-20T03:17:39.465986Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from u1_model_eager import GaugeModelEager, train_one_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T03:21:20.288200Z",
     "start_time": "2018-12-20T03:21:20.195100Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from lattice.gauge_lattice import GaugeLattice, u1_plaq_exact\n",
    "import l2hmc_eager.gauge_dynamics_eager as gde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T03:21:24.656486Z",
     "start_time": "2018-12-20T03:21:24.532910Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lattice = GaugeLattice(time_size=8, \n",
    "                       space_size=8, \n",
    "                       dim=2, \n",
    "                       beta=8., \n",
    "                       link_type='U1',\n",
    "                       num_samples=2,\n",
    "                       rand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T03:21:33.682086Z",
     "start_time": "2018-12-20T03:21:33.548139Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lattice.calc_plaq_observables(lattice.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T03:22:38.829633Z",
     "start_time": "2018-12-20T03:22:38.680817Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lattice.total_action(lattice.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T23:58:32.780512Z",
     "start_time": "2018-12-19T23:58:32.677831Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "potential_fn = lattice.get_energy_function(lattice.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T05:22:29.154891Z",
     "start_time": "2018-12-20T05:22:28.646631Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dynamics = gde.GaugeDynamicsEager(lattice=lattice,\n",
    "                                  num_steps=10,\n",
    "                                  eps=0.1,\n",
    "                                  minus_loglikelihood_fn=potential_fn,\n",
    "                                  conv_net=True,\n",
    "                                  hmc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T05:22:36.415594Z",
     "start_time": "2018-12-20T05:22:36.294196Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "samples_tensor = tf.convert_to_tensor(lattice.samples, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T05:22:40.997199Z",
     "start_time": "2018-12-20T05:22:40.863549Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(samples_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T05:22:50.746961Z",
     "start_time": "2018-12-20T05:22:50.609473Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset.output_shapes, dataset.output_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T15:13:12.981827Z",
     "start_time": "2018-12-19T15:13:12.756364Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(dynamics.apply_transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Iterate over different `eps` and `n_steps`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-09T12:05:37.023Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "eps_arr = np.linspace(0.02, 0.2, 10)\n",
    "n_steps_arr = [5, 10, 15]\n",
    "\n",
    "for eps in eps_arr:\n",
    "    for n_steps in n_steps_arr:\n",
    "        params['eps'] = eps\n",
    "        params['n_steps'] = n_steps\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        model = GaugeModelEager(params=params,\n",
    "                                log_dir=None,\n",
    "                                restore=False,\n",
    "                                use_defun=True)\n",
    "        _ = model.calc_observables(model.samples, _print=True, _write=True)\n",
    "        model.train(1)\n",
    "        \n",
    "        model.train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T10:11:33.286553Z",
     "start_time": "2018-12-07T10:11:33.089310Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T10:06:16.335519Z",
     "start_time": "2018-12-07T10:06:16.034332Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_plaq_arr = np.array(model.average_plaquettes_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T10:06:21.421407Z",
     "start_time": "2018-12-07T10:06:21.320085Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_plaq_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T10:08:14.301027Z",
     "start_time": "2018-12-07T10:08:14.218606Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_plaq_arr.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T10:08:27.460012Z",
     "start_time": "2018-12-07T10:08:27.369277Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "u1_plaq_exact(params['beta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T09:23:10.389991Z",
     "start_time": "2018-12-07T09:15:49.693066Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Graph mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T10:17:19.916127Z",
     "start_time": "2018-12-13T10:17:19.767180Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'time_size': 8,\n",
    "    'space_size': 8,\n",
    "    'link_type': 'U1',\n",
    "    'dim': 2,\n",
    "    'beta': 3.5,\n",
    "    'num_samples': 2,\n",
    "    'n_steps': 5,\n",
    "    'eps': 0.05,\n",
    "    'loss_scale': 0.1,\n",
    "    'loss_eps': 1e-4,\n",
    "    'learning_rate_init': 1e-4,\n",
    "    'learning_rate_decay_steps': 100,\n",
    "    'learning_rate_decay_rate': 0.96,\n",
    "    'train_steps': 1000,\n",
    "    'record_loss_every': 50,\n",
    "    'data_steps': 10,\n",
    "    'save_steps': 50,\n",
    "    'print_steps': 1,\n",
    "    'logging_steps': 5,\n",
    "    'clip_value': 100,\n",
    "    'rand': False,\n",
    "    'metric': 'l2',\n",
    "    'conv_net': True,\n",
    "    'hmc': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T10:17:19.916127Z",
     "start_time": "2018-12-13T10:17:19.767180Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def graph_step(dynamics, optimizer, samples, params, global_step=None):\n",
    "    clip_val = params.get('clip_value', 10)\n",
    "    loss, samples_out, accept_prob, grads = gde.loss_and_grads(\n",
    "        dynamics=dynamics, \n",
    "        x=samples, \n",
    "        params=params,\n",
    "        loss_fn=gde.compute_loss,\n",
    "        defun=False\n",
    "    )\n",
    "    gradients, _ = tf.clip_by_global_norm(grads, clip_val)\n",
    "    train_op = optimizer.apply_gradients(zip(gradients, dynamics.variables))\n",
    "    return train_op, loss, samples_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T10:17:19.916127Z",
     "start_time": "2018-12-13T10:17:19.767180Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    lattice = GaugeLattice(time_size=params.get('time_size', 8),\n",
    "                           space_size=params.get('space_size', 8),\n",
    "                           dim=params.get('dim', 2),\n",
    "                           beta=params.get('beta', '2.5'),\n",
    "                           link_type=params.get('link_type', 'U1'),\n",
    "                           num_samples=params.get('num_samples', 2),\n",
    "                           rand=params.get('rand', False))\n",
    "    batch_size = lattice.samples.shape[0]\n",
    "    samples = tf.convert_to_tensor(\n",
    "        lattice.samples.reshape((batch_size, -1)),\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "    potential_fn = lattice.get_energy_function(samples)\n",
    "\n",
    "    dynamics =  gde.GaugeDynamicsEager(lattice=lattice,\n",
    "                                       n_steps=params.get('n_steps', 10),\n",
    "                                       eps=params.get('eps', 0.1),\n",
    "                                       minus_loglikelihood_fn=potential_fn,\n",
    "                                       conv_net=params.get('conv_net', True),\n",
    "                                       hmc=params.get('hmc', False),\n",
    "                                       defun=False)\n",
    "    x = tf.placeholder(tf.float32, shape=samples.shape)\n",
    "    x_, v_, x_accept_prob, x_out = dynamics.apply_transition(x)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        np_x_, np_v_, np_x_accept_prob, np_x_out =  sess.run(\n",
    "            [x_, v_, x_accept_prob, x_out],\n",
    "            feed_dict={x: lattice.samples.reshape((batch_size, -1))}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T10:17:19.916127Z",
     "start_time": "2018-12-13T10:17:19.767180Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "#with tf.Graph().as_default():\n",
    "lattice = GaugeLattice(time_size=params.get('time_size', 8),\n",
    "                       space_size=params.get('space_size', 8),\n",
    "                       dim=params.get('dim', 2),\n",
    "                       beta=params.get('beta', '2.5'),\n",
    "                       link_type=params.get('link_type', 'U1'),\n",
    "                       num_samples=params.get('num_samples', 2),\n",
    "                       rand=params.get('rand', False))\n",
    "batch_size = lattice.samples.shape[0]\n",
    "samples_tensor = tf.convert_to_tensor(\n",
    "    lattice.samples.reshape((batch_size, -1)),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "potential_fn = lattice.get_energy_function(samples_tensor)\n",
    "\n",
    "dynamics =  gde.GaugeDynamicsEager(lattice=lattice,\n",
    "                                   n_steps=params.get('n_steps', 10),\n",
    "                                   eps=params.get('eps', 0.1),\n",
    "                                   minus_loglikelihood_fn=potential_fn,\n",
    "                                   conv_net=params.get('conv_net', True),\n",
    "                                   hmc=params.get('hmc', False),\n",
    "                                   defun=False)\n",
    "\n",
    "samples = lattice.samples.reshape((batch_size, -1))\n",
    "x = tf.placeholder(tf.float32, shape=samples_tensor.shape)\n",
    "loss, _, _ = gde.compute_loss(dynamics, x, params, defun=False)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=params['learning_rate_init'])\n",
    "train_op, loss, x_out = graph_step(dynamics, optimizer, x, params)\n",
    "\n",
    "session_conf = tf.ConfigProto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T10:17:19.916127Z",
     "start_time": "2018-12-13T10:17:19.767180Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#with tf.Session(config=session_conf) as sess:\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Warmup to reduce initialization effect when timing\n",
    "for _ in range(1):\n",
    "    _, _ = sess.run([train_op, loss], feed_dict={x: samples})\n",
    "\n",
    "# Training\n",
    "#start_time = time.time()\n",
    "for i in range(100):\n",
    "    t0 = time.time()\n",
    "    _, loss_np, samples = sess.run([train_op, loss, x_out],\n",
    "                                       feed_dict={x: samples})\n",
    "    print(f\"step: {i}  loss: {loss_np:^6.4g}  time/step: {time.time() - t0:^6.4g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T04:15:31.092264Z",
     "start_time": "2018-12-07T04:15:31.022733Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train_one_iter(dynamics, x, optimizer, loss_fn=l2hmc.compute_loss, \n",
    "                   scale=0.1, eps=1e-4, metric='l2', clip_value=10, \n",
    "                   global_step=None):\n",
    "    loss, grads, out, accept_prob = l2hmc.loss_and_grads(\n",
    "        dynamics, x, loss_fn=loss_fn, scale=scale, eps=eps, metric=metric\n",
    "    )\n",
    "    gradients, _ = tf.clip_by_global_norm(grads, clip_value)\n",
    "    optimizer.apply_gradients(\n",
    "        zip(grads, dynamics.trainable_variables), global_step=global_step\n",
    "    )\n",
    "    return loss, out, accept_prob, gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T04:15:31.998767Z",
     "start_time": "2018-12-07T04:15:31.931377Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def write_run_data(file_path, data, write_mode='a'):\n",
    "    with open(file_path, write_mode) as f:\n",
    "        f.write('\\n')\n",
    "        info_str = (f\"step: {data['step']:<3g} loss: {data['loss']:^6.5g} \"\n",
    "                    f\" time/step: {data['time']:^6.5g} \"\n",
    "                    f\" accept: {data['accept_prob']:^6.5g} \"\n",
    "                    f\" eps: {data['eps']:^6.5g} \"\n",
    "                    f\" avg_S: {data['avg_S']:^6.5g} \"\n",
    "                    f\" avg_topQ: {data['avg_top_charge']:^6.5g} \"\n",
    "                    f\" avg_plaq: {data['avg_plaq']:^6.5g} \\n\")\n",
    "        f.write(info_str)\n",
    "        f.write('\\n')\n",
    "        f.write('avg_plaquettes: {}\\n'.format(data['avg_plaquettes']))\n",
    "        f.write('topological_charges: {}\\n'.format(data['top_charges']))\n",
    "        f.write('total_actions: {}\\n'.format(data['total_actions']))\n",
    "        f.write((len(info_str) + 1)*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T04:15:32.869957Z",
     "start_time": "2018-12-07T04:15:32.756150Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def write_run_parameters(file_path, parameters):\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write('Parameters:\\n')\n",
    "        f.write(80 * '-' + '\\n')\n",
    "        for key, val in parameters.items():\n",
    "            f.write(f'{key}: {val}\\n')\n",
    "        f.write(80*'=')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T04:15:33.817161Z",
     "start_time": "2018-12-07T04:15:33.733607Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def print_run_data(data):\n",
    "    print(f\"\\nstep: {data['step']:<3g} loss: {data['loss']:^6.5g} \"\n",
    "          f\" time/step: {data['time']:^6.5g} \"\n",
    "          f\" accept: {data['accept_prob']:^6.5g} \"\n",
    "          f\" eps: {data['eps']:^6.5g} \"\n",
    "          f\" avg_S: {data['avg_S']:^6.5g} \"\n",
    "          f\" avg_topQ: {data['avg_top_charge']:^6.5g} \"\n",
    "          f\" avg_plaq: {data['avg_plaq']:^6.5g} \\n\")\n",
    "    print('avg_plaquettes: {}\\n'.format(data['avg_plaquettes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T04:15:34.749894Z",
     "start_time": "2018-12-07T04:15:34.650699Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def save_run_data(checkpointer, log_dir, files, data):\n",
    "    saved_path = checkpointer.save(file_prefix=os.path.join(log_dir,\n",
    "                                                            \"ckpt\"))\n",
    "    print(f\"Saved checkpoint to: {saved_path}\")\n",
    "    np.save(files['avg_plaquettes_file'], data['avg_plaquettes_arr'])\n",
    "    np.save(files['total_actions_file'], data['total_actions_arr'])\n",
    "    np.save(files['top_charges_file'], data['top_charges_arr'])\n",
    "    np.save(files['samples_file'], data['samples'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T04:15:35.925637Z",
     "start_time": "2018-12-07T04:15:35.834779Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def write_summaries(summary_writer, data):\n",
    "    with summary_writer.as_default():\n",
    "        with tf.contrib.summary.always_record_summaries():\n",
    "            tf.contrib.summary.scalar(\"Training_loss\", data['loss'],\n",
    "                                      step=data['global_step'])\n",
    "            tf.contrib.summary.scalar(\"avg_plaquettes\", data['avg_plaq'],\n",
    "                                      step=data['global_step'])\n",
    "            tf.contrib.summary.scalar(\"total_actions\", data['avg_S'],\n",
    "                                      step=data['global_step'])\n",
    "            tf.contrib.summary.scalar(\"top_charges\", data['avg_top_charge'],\n",
    "                                      step=data['global_step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T04:15:36.932143Z",
     "start_time": "2018-12-07T04:15:36.754177Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def exact_plaquette_average(beta):\n",
    "    return i1(beta) / i0(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct GaugeLattice with $U(1)$ gauge group\n",
    "\n",
    "$$ S[\\theta] = \\beta \\sum_{x;\\,\\, \\nu\\neq\\mu} 1 - \\cos(\\theta_{\\mu\\nu})$$\n",
    "with $\\theta_{\\mu\\nu} \\equiv \\theta_{\\mu}(x) + \\theta_{\\nu}(x +\\hat \\mu) - \\theta_{\\mu}(x + \\hat \\nu) - \\theta_{\\nu}(x)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run L2HMC for $U(1)$ gauge model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T04:15:42.029142Z",
     "start_time": "2018-12-07T04:15:41.945328Z"
    },
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "##########################  Parameters  #####################################\n",
    "# n_steps: number of leapfrog steps, eps: initial step size for dynamics\n",
    "# loss_scale: scaling factor (lambda^2 in paper) in loss objective\n",
    "# loss_eps: for numeric stability in loss function\n",
    "# beta: inverse coupling strength\n",
    "##############################################################################\n",
    "params = {\n",
    "    'time_size': 8,\n",
    "    'space_size': 8,\n",
    "    'dim': 2,\n",
    "    'beta': 3.5,\n",
    "    'num_samples': 2,\n",
    "    'n_steps': 10,\n",
    "    'eps': 0.05,\n",
    "    'loss_scale': 0.1,\n",
    "    'loss_eps': 1e-4,\n",
    "    'learning_rate': 1e-4,\n",
    "    'learning_rate_decay_steps': 100,\n",
    "    'learning_rate_decay_rate': 0.96,\n",
    "    'train_iters': 500,\n",
    "    'record_loss_every': 50,\n",
    "    'save_steps': 50,\n",
    "    'clip_value': 100,\n",
    "    'rand': False,\n",
    "    'conv_net': True,\n",
    "    'metric': 'l2'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T04:18:42.397669Z",
     "start_time": "2018-12-07T04:18:41.958650Z"
    },
    "code_folding": [
     17
    ]
   },
   "outputs": [],
   "source": [
    "u1_lattice = GaugeLattice(time_size=params['time_size'], \n",
    "                          space_size=params['space_size'], \n",
    "                          dim=params['dim'], \n",
    "                          beta=params['beta'],\n",
    "                          link_type='U1', \n",
    "                          num_samples=params['num_samples'], \n",
    "                          rand=params['rand'])\n",
    "if params['conv_net']:\n",
    "    u1_samples_tensor = tf.convert_to_tensor(u1_lattice.samples, \n",
    "                                             dtype=tf.float32)\n",
    "else:\n",
    "    flat_samples = [sample.flatten() for sample in u1_lattice.samples]\n",
    "    u1_samples_tensor = tf.convert_to_tensor(np.stack(flat_samples), \n",
    "                                             dtype=tf.float32)\n",
    "\n",
    "# Construct dynamics object\n",
    "u1_energy_fn = u1_lattice.get_energy_function(u1_samples_tensor)\n",
    "u1_dynamics = l2hmc.GaugeDynamicsEager(lattice=u1_lattice, \n",
    "                                       n_steps=params['n_steps'],\n",
    "                                       eps=params['eps'],\n",
    "                                       minus_loglikelihood_fn=u1_energy_fn, \n",
    "                                       conv_net=params['conv_net'], \n",
    "                                       hmc=False)\n",
    "total_actions = []\n",
    "average_plaquettes = []\n",
    "topological_charges = []\n",
    "samples = u1_samples_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T04:18:50.756848Z",
     "start_time": "2018-12-07T04:18:50.656417Z"
    },
    "code_folding": [
     0,
     2,
     13,
     17
    ]
   },
   "outputs": [],
   "source": [
    "# Create new log_dir with new run number\n",
    "root_dir = '../../U1_logs/'\n",
    "if not os.path.exists(root_dir):\n",
    "    os.makedirs(root_dir)\n",
    "\n",
    "log_dirs = os.listdir(root_dir)\n",
    "try:\n",
    "    run_nums = [int(i.split('_')[-1]) for i in log_dirs if i.startswith('run')]\n",
    "    run_num = max(run_nums) + 1\n",
    "except:\n",
    "    run_num = 1\n",
    "    \n",
    "log_dir = f'../../U1_logs/run_{run_num}'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "print(f'{log_dir}')\n",
    "    \n",
    "run_files = {\n",
    "    'parameters_file': os.path.join(log_dir, 'parameters.txt'),\n",
    "    'run_info_file': os.path.join(log_dir, 'run_info.txt'),\n",
    "    'avg_plaquettes_file':  os.path.join(log_dir, 'average_plaquettes.npy'),\n",
    "    'total_actions_file': os.path.join(log_dir, 'total_actions.npy'),\n",
    "    'top_charges_file': os.path.join(log_dir, 'topological_charges.npy'),\n",
    "    'samples_file': os.path.join(log_dir, 'samples.npy'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T04:19:00.824310Z",
     "start_time": "2018-12-07T04:19:00.724369Z"
    },
    "code_folding": [
     3,
     9
    ]
   },
   "outputs": [],
   "source": [
    "global_step = tf.train.get_or_create_global_step()\n",
    "_ = global_step.assign(1)\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(learning_rate=params['learning_rate'], \n",
    "                                           global_step=global_step, \n",
    "                                           decay_steps=params['learning_rate_decay_steps'],\n",
    "                                           decay_rate=params['learning_rate_decay_rate'], \n",
    "                                           staircase=True)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "checkpointer = tf.train.Checkpoint(\n",
    "    optimizer=optimizer, dynamics=u1_dynamics, global_step=global_step\n",
    ")\n",
    "summary_writer = tf.contrib.summary.create_file_writer(log_dir)\n",
    "loss_fn = l2hmc.compute_loss\n",
    "\n",
    "print(u1_plaq_exact(params['beta']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T04:19:06.078659Z",
     "start_time": "2018-12-07T04:19:05.742811Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "################### SAVE INFO ABOUT INITIAL STATE OF LATTICE ###################\n",
    "start_step = global_step.numpy()\n",
    "observables = np.array(u1_lattice.calc_plaq_observables(samples))\n",
    "_total_actions = observables[:, 0]\n",
    "_avg_plaquettes = observables[:, 1]\n",
    "_top_charges = observables[:, 2]\n",
    "\n",
    "total_actions.append(_total_actions)\n",
    "average_plaquettes.append(_avg_plaquettes)\n",
    "topological_charges.append(_top_charges)\n",
    "\n",
    "data = {\n",
    "    'step': 0,\n",
    "    'global_step': global_step.numpy(),\n",
    "    'loss': 0.,\n",
    "    'time': 0.,\n",
    "    'accept_prob': 0.,\n",
    "    'eps': u1_dynamics.eps.numpy(),\n",
    "    'avg_S': np.mean(_total_actions),\n",
    "    'avg_top_charge': np.mean(_top_charges),\n",
    "    'avg_plaq': np.mean(_avg_plaquettes),\n",
    "    'avg_plaquettes': _avg_plaquettes,\n",
    "    'top_charges': _top_charges,\n",
    "    'total_actions': _total_actions,\n",
    "    'avg_plaquettes_arr': average_plaquettes,\n",
    "    'top_charges_arr': topological_charges,\n",
    "    'total_actions_arr': total_actions,\n",
    "    'samples': samples.numpy()\n",
    "}\n",
    "\n",
    "_ = print_run_data(data)\n",
    "_ = write_run_data(run_files['run_info_file'], data, 'w')\n",
    "_ = write_run_parameters(run_files['parameters_file'], params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-24T10:36:42.761386Z",
     "start_time": "2018-11-23T23:48:49.046921Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "############################ RUN L2HMC ALGORITHM ############################\n",
    "t0 = time.time()\n",
    "for i in range(start_step, 1000):\n",
    "    t1 = time.time()\n",
    "    loss, samples, accept_prob, grads = train_one_iter(\n",
    "        u1_dynamics,\n",
    "        samples,\n",
    "        optimizer,\n",
    "        loss_fn=loss_fn,\n",
    "        scale=loss_scale,\n",
    "        metric=metric,\n",
    "        eps=loss_eps,\n",
    "        clip_value=clip_value,\n",
    "        global_step=global_step\n",
    "    )\n",
    "    observables = np.array(u1_lattice.calc_plaq_observables(samples))\n",
    "    _total_actions = observables[:, 0]\n",
    "    _avg_plaquettes = observables[:, 1]\n",
    "    _top_charges = observables[:, 2]\n",
    "    \n",
    "    total_actions.append(_total_actions)\n",
    "    average_plaquettes.append(_avg_plaquettes)\n",
    "    topological_charges.append(_top_charges)\n",
    "    \n",
    "    data = {\n",
    "        'step': i,\n",
    "        'global_step': global_step.numpy(),\n",
    "        'loss': loss.numpy(),\n",
    "        'time': time.time() - t1,\n",
    "        'accept_prob': accept_prob.numpy().mean(),\n",
    "        'eps': u1_dynamics.eps.numpy(),\n",
    "        'avg_S': np.mean(_total_actions),\n",
    "        'avg_top_charge': np.mean(_top_charges),\n",
    "        'avg_plaq': np.mean(_avg_plaquettes),\n",
    "        'avg_plaquettes': _avg_plaquettes,\n",
    "        'top_charges': _top_charges,\n",
    "        'total_actions': _total_actions,\n",
    "        'avg_plaquettes_arr': np.array(average_plaquettes),\n",
    "        'top_charges_arr': np.array(topological_charges),\n",
    "        'total_actions_arr': np.array(total_actions),\n",
    "        'samples': samples.numpy()\n",
    "    }\n",
    "    \n",
    "    _ = write_run_data(run_files['run_info_file'], data, 'a')\n",
    "    _ = print_run_data(data)\n",
    "    \n",
    "    if i % record_loss_every == 0:\n",
    "        write_summaries(summary_writer, data)\n",
    "\n",
    "    if i % save_steps == 0:\n",
    "        save_run_data(checkpointer, log_dir, run_files, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T23:19:45.854829Z",
     "start_time": "2018-11-23T23:19:45.190515Z"
    }
   },
   "outputs": [],
   "source": [
    "avg_plaqs_arr = np.array(average_plaquettes)\n",
    "_avg_plaqs_arr = np.mean(avg_plaqs_arr, axis=0)\n",
    "avg_plaq, avg_plaq_err = calc_avg_vals_errors(avg_plaqs_arr[-400:], num_blocks=10)\n",
    "print(f'avg_plaq (mean from arr): {np.mean(_avg_plaqs_arr)}')\n",
    "print(f'avg_plaq: {avg_plaq} +/- {avg_plaq_err}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-17T20:40:37.713638Z",
     "start_time": "2018-11-17T20:40:37.632081Z"
    }
   },
   "outputs": [],
   "source": [
    "calc_avg_vals_errors(_avg_plaqs_arr, num_blocks=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Gradients check\n",
    "\n",
    "Want to test that the value of the gradient ($\\partial_x U(x)$) is the same when  \n",
    "calculated using either:\n",
    "1. Tensorflow's automatic differentiation (`tf.gradients`), or\n",
    "2. The analytic expression (Eq. 17) from [reference](https://arxiv.org/pdf/hep-lat/9809160.pdf):\n",
    "$$\\partial_x U(x) = \\beta \\sum_{\\nu \\neq \\mu} \\sin \\theta_{\\mu\\nu}(x - \\hat\\nu) - \\sin \\theta_{\\mu\\nu}(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T20:21:53.806280Z",
     "start_time": "2018-11-07T20:21:53.744943Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# fake data used to compare values of the gradient of the potential function\n",
    "samples_ = tf.convert_to_tensor(\n",
    "    [np.arange(128).reshape(u1_lattice.links.shape) for _ in range(4)],\n",
    "    dtype=tf.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T20:22:38.437759Z",
     "start_time": "2018-11-07T20:22:36.009619Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# using tensorflow's automatic differentiation \n",
    "tf_grad = u1_dynamics.grad_potential(samples) \n",
    "# 276 ms  15.4 ms per loop (mean  std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T20:22:45.110380Z",
     "start_time": "2018-11-07T20:22:40.767834Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# using the analytic expression\n",
    "# (Eq. 17 from https://arxiv.org/pdf/hep-lat/9809160.pdf)\n",
    "exact_grad = u1_lattice.grad_action(samples)\n",
    "# 503 ms  33.9 ms per loop (mean  std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T20:22:12.648092Z",
     "start_time": "2018-11-07T20:22:12.593599Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('exact grad:')\n",
    "print(exact_grad[0].reshape(u1_lattice.links.shape)[:4, :4, 0])\n",
    "print('\\n')\n",
    "print('tf grad:')\n",
    "print(tf_grad[0].numpy().reshape(u1_lattice.links.shape)[:4, :4, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T20:23:02.821862Z",
     "start_time": "2018-11-07T20:23:02.755580Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('exact grad:')\n",
    "print(exact_grad[0].reshape(u1_lattice.links.shape)[:4, :4, 1])\n",
    "print('\\n')\n",
    "print('tf grad:')\n",
    "print(tf_grad[0].numpy().reshape(u1_lattice.links.shape)[:4, :4, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T11:21:32.820691Z",
     "start_time": "2018-11-15T11:21:32.744753Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "################  tests  #############################\n",
    "#u1_lattice._total_action()\n",
    "#u1_lattice._average_plaquette()\n",
    "#u1_lattice.total_action(u1_samples)\n",
    "#u1_lattice.total_action()\n",
    "#u1_lattice.average_plaquette(u1_samples)\n",
    "#u1_lattice.average_plaquette()\n",
    "#---------------------------------------------\n",
    "#_momentum = tf.random_normal(tf.shape(u1_samples))\n",
    "#_potential = np.array(u1_dynamics.potential(u1_samples_tensor))\n",
    "#_kinetic = u1_dynamics.kinetic(_momentum)\n",
    "#_grad_potential = u1_dynamics.grad_potential(u1_samples_tensor)\n",
    "#print(_potential); print('\\n')\n",
    "#print(_kinetic); print('\\n')\n",
    "#print(_grad_potential)\n",
    "#print(_kinetic.numpy()); print('\\n')\n",
    "#print(_hamiltonian.numpy()); print('\\n')\n",
    "#print(_grad_potential[0][:10])\n",
    "#---------------------------------------------\n",
    "#site = u1_lattice.get_random_site()\n",
    "#u = np.random.randint(u1_lattice.dim)\n",
    "#v = np.random.randint(u1_lattice.dim)\n",
    "#plaq = u1_lattice.plaquette_operator(site, u, v)\n",
    "#---------------------------------------------\n",
    "#u1_lattice.total_action(u1_samples_tensor)\n",
    "#u1_lattice.average_plaquette(u1_samples_tensor)\n",
    "#transition_out = u1_dynamics.apply_transition(u1_samples_tensor)\n",
    "#x_post, p_post, accept_prob, x_out = transition_out\n",
    "#loss, x_out, x_accept_prob = l2hmc.compute_loss(u1_dynamics, u1_samples_tensor)\n",
    "#x_accept_prob\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Heatbath Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T21:37:21.878798Z",
     "start_time": "2018-11-03T21:37:21.812692Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "time_size, space_size, dim, beta, num_samples = (16, 16, 2, 4., 5)\n",
    "u1_lattice = GaugeLattice(time_size, space_size, dim, beta,\n",
    "                          link_type='U1', num_samples=num_samples)\n",
    "u1_samples = [sample.flatten() for sample in u1_lattice.samples]\n",
    "u1_samples_tensor = tf.constant(np.stack(u1_samples), dtype=tf.float32)\n",
    "\n",
    "eq_steps = 5000\n",
    "acceptances = []\n",
    "action_arr = [u1_lattice._total_action()]\n",
    "avg_plaq_arr = [u1_lattice._average_plaquette()]\n",
    "for i in range(eq_steps):\n",
    "    action = u1_lattice._total_action()\n",
    "    avg_plaq = u1_lattice._average_plaquette()\n",
    "    change = avg_plaq - avg_plaq_arr[-1]\n",
    "    avg_plaq_arr.append(avg_plaq)\n",
    "    action_arr.append(action)\n",
    "    print(f\"Step: {i:<5g}\\t action: {action:<8.4g}\\t \"\n",
    "          f\"avg plaq: {avg_plaq:<8.4g}\\t change: {change:<8.4g}\")\n",
    "    accept = 0\n",
    "    for site in u1_lattice.iter_sites():\n",
    "        for d in range(u1_lattice.dim):\n",
    "            accept += u1_lattice._update_link(site, d)\n",
    "    acceptances.append(accept)\n",
    "# 12.2s for 500 equilibration steps\n",
    "\n",
    "avg_plaq_arr = [0]\n",
    "p = k - j\n",
    "for k in range(j, 40000):\n",
    "    avg_plaq = u1_lattice._average_plaquette()\n",
    "    change = avg_plaq - avg_plaq_arr[p-1]\n",
    "    avg_plaq_arr.append(avg_plaq)\n",
    "    print(f\"Step: {k:<5g}: avg plaq: {avg_plaq:>12.4g} change: {change:12.4g}\")\n",
    "    for site in u1_lattice.iter_sites():\n",
    "        for d in range(u1_lattice.dim):\n",
    "            _ = u1_lattice._update_link(site, d)\n",
    "\n",
    "num_acceptances = 0\n",
    "measure_steps = 10000\n",
    "avg_plq = np.zeros(measure_steps)\n",
    "for step in range(measure_steps):\n",
    "    for site in u1_lattice.iter_sites():\n",
    "        for d in range(u1_lattice.dim):\n",
    "            num_acceptances += u1_lattice._update_link(site, d)\n",
    "    avg_plq[step] = u1_lattice._average_plaquette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## OLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### GMM Model (test L2HMC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T20:28:31.007387Z",
     "start_time": "2018-10-31T20:28:30.930207Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sigmas, distribution = gen_ring(1., var=0.02, nb_mixtures=4)\n",
    "\n",
    "gmm_potential = distribution.get_energy_function()\n",
    "gmm_dynamics = _l2hmc.Dynamics(x_dim=2, minus_loglikelihood_fn=gmm_potential,\n",
    "                               n_steps=25, eps=0.1)\n",
    "\n",
    "samples = distribution.get_samples(200)\n",
    "\n",
    "_position = tf.convert_to_tensor(samples, dtype=tf.float32)\n",
    "_momentum = tf.random_normal(tf.shape(_position))\n",
    "_hamiltonian = gmm_dynamics.hamiltonian(_position, _momentum)\n",
    "\n",
    "grad_pot = gmm_dynamics.grad_potential(_position, _momentum)\n",
    "\n",
    "grad_pot.shape\n",
    "\n",
    "gmm_dynamics.masks[0]\n",
    "\n",
    "scale, translation, transformed = gmm_dynamics.position_fn([_position, _momentum, gmm_dynamics._get_time(0)])\n",
    "\n",
    "scale.shape\n",
    "\n",
    "_position.shape\n",
    "\n",
    "gmm_dynamics.masks[0].shape\n",
    "\n",
    "gmm_dynamics.masks[0] * _position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Construct GaugeLattice with SU(3) gauge group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-26T15:12:32.733478Z",
     "start_time": "2018-09-26T15:12:32.504656Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "time_size = 2\n",
    "space_size = 4\n",
    "dim = 4\n",
    "beta = 1.\n",
    "link_type = 'SU3' \n",
    "batch_size = 3\n",
    "gauge_lattice = GaugeLattice(time_size, space_size, dim, beta, link_type)\n",
    "# create `num_samples` random samples of GaugeLattice.links\n",
    "links_samples = gauge_lattice.get_links_samples(batch_size, link_type=link_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-26T15:12:40.607989Z",
     "start_time": "2018-09-26T15:12:40.489217Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gauge_energy_fn = gauge_lattice.get_energy_function()\n",
    "gauge_dynamics = l2hmc.GaugeDynamics(gauge_lattice, \n",
    "                                     minus_loglikelihood_fn=gauge_energy_fn, \n",
    "                                     batch_size=3, n_steps=5, eps=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-26T15:14:28.743723Z",
     "start_time": "2018-09-26T15:14:28.662992Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gauge_lattice.links.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-26T15:12:45.079360Z",
     "start_time": "2018-09-26T15:12:43.109499Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "potential_arr = gauge_dynamics.potential(links_samples, batch_size)\n",
    "\n",
    "[i.numpy() for i in potential_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-26T15:13:16.255416Z",
     "start_time": "2018-09-26T15:13:16.157588Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_momentum = tf.random_normal(tf.shape(links_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-26T15:13:23.283119Z",
     "start_time": "2018-09-26T15:13:23.204441Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gauge_dynamics.kinetic(_momentum).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-26T15:13:37.712836Z",
     "start_time": "2018-09-26T15:13:35.821847Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_x = links_samples\n",
    "#_momentum = tf.random_normal(tf.shape(_x))\n",
    "_hamiltonian = gauge_dynamics.hamiltonian(_x, _momentum)\n",
    "_hamiltonian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Construct IsingLattice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-26T06:37:22.328501Z",
     "start_time": "2018-09-26T06:37:22.220884Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ising_batch_size = 10\n",
    "ising_lattice = IsingLattice(3, 4)\n",
    "ising_samples = [ising_lattice._randomize() for _ in range(ising_batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-26T06:37:51.184935Z",
     "start_time": "2018-09-26T06:37:51.073172Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ising_energy_fn = ising_lattice.get_energy_function()\n",
    "ising_dynamics = l2hmc.LatticeDynamics(ising_lattice, \n",
    "                                       minus_loglikelihood_fn=ising_energy_fn,\n",
    "                                       batch_size=ising_batch_size, \n",
    "                                       n_steps=10, eps=0.1)\n",
    "#dynamics = l2hmc.LDynamics(latt.sites.shape, minus_loglikelihood_fn=energy_fn, n_steps=10, eps=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-26T06:37:54.524077Z",
     "start_time": "2018-09-26T06:37:54.435783Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ising_dynamics.potential(samples, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-26T06:38:19.182664Z",
     "start_time": "2018-09-26T06:38:19.078701Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_iposition = ising_samples\n",
    "_imomentum = tf.random_normal(tf.shape(_iposition))\n",
    "_ihamiltonian = dynamics.hamiltonian(_iposition, _imomentum)\n",
    "_ihamiltonian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-26T06:38:30.265961Z",
     "start_time": "2018-09-26T06:38:30.174827Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_isample = _iposition[0].reshape(ising_lattice.num_sites)\n",
    "#dynamics.grad_potential(np.array(_position).reshape(-1, lattice.num_sites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-26T06:50:33.496377Z",
     "start_time": "2018-09-26T06:50:32.749093Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "grad_pot = dynamics.grad_potential(ising_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-26T06:50:55.836732Z",
     "start_time": "2018-09-26T06:50:55.711166Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ising_jacobian = jacobian(dynamics.potential, ising_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-26T05:56:57.584436Z",
     "start_time": "2018-09-26T05:56:57.504478Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "grad_fn = tfe.gradients_function(lattice._calc_energy, params=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-26T06:36:03.157629Z",
     "start_time": "2018-09-26T06:36:03.059724Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_jacobian = jacobian(dynamics.potential, _position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-26T05:56:58.246252Z",
     "start_time": "2018-09-26T05:56:58.161898Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lattice.calc_energy(_position, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-26T05:57:00.976505Z",
     "start_time": "2018-09-26T05:57:00.753293Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#dynamics.position_fn(momentum, latt.sites.flatten()[:], dynamics)\n",
    "#dynamics._forward_lf(latt.sites.flatten()[:], momentum, 0)\n",
    "dynamics._forward_lf(np.array(_position).reshape(-1, lattice.num_sites),\n",
    "                     np.array(_momentum).reshape(-1, lattice.num_sites), 1)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "notify_time": "5",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
