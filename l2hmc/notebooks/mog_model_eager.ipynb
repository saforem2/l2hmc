{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2HMC with MOG target distrubtion using eager execution in tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from l2hmc_eager import dynamics_eager as l2hmc\n",
    "from l2hmc_eager.neural_nets import *\n",
    "from utils.distributions import GMM, gen_ring\n",
    "from utils.jacobian import _map, jacobian\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()\n",
    "tfe = tf.contrib.eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfe = tf.contrib.eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_iter(dynamics, x, optimizer, \n",
    "                   loss_fn=l2hmc.compute_loss, global_step=None):\n",
    "    loss, grads, out, accept_prob = l2hmc.loss_and_grads(\n",
    "        dynamics, x, loss_fn=loss_fn\n",
    "    )\n",
    "    optimizer.apply_gradients(\n",
    "        zip(grads, dynamics.trainable_variables), global_step=global_step\n",
    "    )\n",
    "    return loss, out, accept_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_arr(x_dim, n_distributions):\n",
    "    \"\"\"Create array describing likelihood of drawing from distributions.\"\"\"\n",
    "    if n_distributions > x_dim:\n",
    "        pis = [1. / n_distributions] * n_distributions\n",
    "        pis[0] += 1 - sum(pis)\n",
    "        return np.array(pis)\n",
    "    if x_dim == n_distributions:\n",
    "        big_pi = round(1.0 / n_distributions, x_dim)\n",
    "        pis = n_distributions * [big_pi]\n",
    "        return np.array(pis)\n",
    "    else:\n",
    "        big_pi = (1.0 / n_distributions) - x_dim * 1E-16\n",
    "        pis = n_distributions * [big_pi]\n",
    "        small_pi = (1. - sum(pis)) / (x_dim - n_distributions)\n",
    "        pis.extend((x_dim - n_distributions) * [small_pi])\n",
    "        return np.array(pis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MoG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dim = 2 \n",
    "num_distributions = 2\n",
    "sigma = 0.05\n",
    "axis = 0\n",
    "centers = 1\n",
    "\n",
    "means = np.zeros((x_dim, x_dim))\n",
    "means[::2, axis] = centers\n",
    "means[1::2, axis] = - centers\n",
    "\n",
    "cov_mtx = sigma * np.eye(x_dim)\n",
    "sigmas = np.array([cov_mtx] * x_dim)\n",
    "\n",
    "pis = distribution_arr(x_dim, num_distributions)\n",
    "mog_distribution = GMM(means, sigmas, pis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mog_potential_fn = mog_distribution.get_energy_function()\n",
    "\n",
    "mog_dynamics = l2hmc.Dynamics(x_dim=2, \n",
    "                              minus_loglikelihood_fn=mog_potential_fn,\n",
    "                              n_steps=2,\n",
    "                              eps=0.5,\n",
    "                              np_seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iters = 1000\n",
    "eval_iters = 20 \n",
    "n_samples = 200\n",
    "record_loss_every = 10 \n",
    "save_steps = 100 \n",
    "\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "global_step.assign(1)\n",
    "learning_rate = tf.train.exponential_decay(1e-3, global_step, \n",
    "                                           1000, 0.96, staircase=True)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "checkpointer = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                   dynamics=mog_dynamics,\n",
    "                                   global_step=global_step)\n",
    "\n",
    "log_dir = '../../tf_eager_log/mog_model/run_3/'\n",
    "summary_writer = tf.contrib.summary.create_file_writer(log_dir)\n",
    "# if restore:\n",
    "#     latest_path = tf.train.latest_checkpoint(train_dir)\n",
    "#     checkpointer.restore(latest_path)\n",
    "#     print(\"Restored latest checkpoint at path:\\\"{}\\\"\".format(latest_path))\n",
    "#     sys.stdout.flush()\n",
    "# if not restore:\n",
    "#     if use_defun:\n",
    "#         loss_fn = tfe.function(l2hmc.compute_loss)\n",
    "#     else:\n",
    "loss_fn = tfe.defun(l2hmc.compute_loss)\n",
    "samples = tf.random_normal(shape=[n_samples, x_dim])\n",
    "for i in range(1, train_iters + 1):\n",
    "    loss, samples, accept_prob = train_one_iter(\n",
    "        mog_dynamics,\n",
    "        samples,\n",
    "        optimizer,\n",
    "        loss_fn=loss_fn,\n",
    "        global_step=global_step\n",
    "    )\n",
    "    \n",
    "    if i % record_loss_every == 0:\n",
    "        print(\"Iteration {}, loss {:.4f}, x_accept_prob {:.4f}\".format(\n",
    "            i, loss.numpy(), accept_prob.numpy().mean()\n",
    "        ))\n",
    "        with summary_writer.as_default():\n",
    "            with tf.contrib.summary.always_record_summaries():\n",
    "                _ = tf.contrib.summary.scalar(\"Training loss\", \n",
    "                                              loss, \n",
    "                                              step=global_step)\n",
    "                \n",
    "    if i % save_steps == 0:\n",
    "        saved_path = checkpointer.save(file_prefix=os.path.join(log_dir,\n",
    "                                                                \"ckpt\"))\n",
    "        print(f\"Saved checkpoint to: {saved_path}\")\n",
    "        \n",
    "print(\"Training complete.\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_samples = tf.random_normal(shape=[n_samples, x_dim])\n",
    "samples_history = []\n",
    "for i in range(100):\n",
    "    samples_history.append(_samples.numpy())\n",
    "    _, _, _, _samples = mog_dynamics.apply_transition(_samples)\n",
    "samples_history = np.array(samples_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_history.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_samples = mog_distribution.get_samples(500)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(target_samples[:,0], target_samples[:,1], color='C0', alpha=0.5, marker='o', ls='')\n",
    "ax.plot(samples_history[:, 0, 0], samples_history[:, 0, 1], color='C1', alpha=0.75, ls='-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = mog_distribution.get_samples(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(samples[:,0], samples[:,1], color='C0', marker='o', ls='')\n",
    "#ax.plot(samples_history[:, 0, 0], samples_history[:, 0, 1], color='C1', alpha=0.75, ls='-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
